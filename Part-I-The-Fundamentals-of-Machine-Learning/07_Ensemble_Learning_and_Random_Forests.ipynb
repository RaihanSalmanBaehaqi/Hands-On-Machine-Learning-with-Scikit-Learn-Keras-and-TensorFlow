{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "author_cell"
      },
      "source": [
        "**AUTHOR: RAIHAN SALMAN BAEHAQI (1103220180)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part_header"
      },
      "source": [
        "**PART I** \n",
        "\n",
        "**The Fundamentals of Machine Learning** \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chapter_header"
      },
      "source": [
        "**CHAPTER 7 - Ensemble Learning and Random Forests** \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "Chapter 7 explores Ensemble Learning, a powerful technique based on the wisdom of the crowd principle. By aggregating predictions from multiple predictors, ensemble methods often achieve better results than individual models. This chapter covers voting classifiers, bagging, boosting, stacking, and Random Forests. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voting_classifiers"
      },
      "source": [
        "**Voting Classifiers**   \n",
        "A simple way to create a better classifier is to aggregate predictions from multiple classifiers and predict the class that gets the most votes. This majority-vote classifier is called a **hard voting classifier**. \n",
        "\n",
        "**Figure 7-1. Training diverse classifiers**   \n",
        "![Figure7-1.jpg](./07.Chapter-07/Figure7-1.jpg) \n",
        "\n",
        "**Figure 7-2. Hard voting classifier predictions**   \n",
        "![Figure7-2.jpg](./07.Chapter-07/Figure7-2.jpg) \n",
        "\n",
        "The voting classifier often achieves higher accuracy than the best classifier in the ensemble. Even if each classifier is a **weak learner** (only slightly better than random guessing), the ensemble can be a **strong learner** (achieving high accuracy), provided there are sufficient weak learners that are sufficiently diverse.\n",
        "\n",
        "**The Law of Large Numbers**   \n",
        "Consider a biased coin with 51% chance of heads and 49% chance of tails. After 1,000 tosses, you'll get approximately 510 heads and 490 tails, with about 75% probability of obtaining a majority of heads. With 10,000 tosses, this probability climbs over 97%. This is due to the **law of large numbers**: as tosses increase, the ratio of heads approaches the true probability (51%). \n",
        "\n",
        "**Figure 7-3. The law of large numbers**   \n",
        "![Figure7-3.jpg](./07.Chapter-07/Figure7-3.jpg) \n",
        "\n",
        "Similarly, an ensemble with 1,000 classifiers individually correct 51% of the time can achieve up to 75% accuracy. However, this assumes perfect independence with uncorrelated errors. Ensemble methods work best when predictors are as independent as possible. One way to get diverse classifiers is to train them using very different algorithms.\n",
        "\n",
        "**Code Example: Voting Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voting_code"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard')\n",
        "voting_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_text"
      },
      "source": [
        "Evaluating each classifier's accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_code"
      },
      "outputs": [],
      "source": [
        ">>> from sklearn.metrics import accuracy_score\n",
        ">>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "...     clf.fit(X_train, y_train)\n",
        "...     y_pred = clf.predict(X_test)\n",
        "...     print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
        "...\n",
        "LogisticRegression 0.864\n",
        "RandomForestClassifier 0.896\n",
        "SVC 0.888\n",
        "VotingClassifier 0.904"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soft_voting"
      },
      "source": [
        "The voting classifier slightly outperforms all individual classifiers. \n",
        "\n",
        "**Soft Voting**   \n",
        "If all classifiers can estimate class probabilities (have a predict_proba() method), you can use **soft voting**: predict the class with the highest class probability averaged over all classifiers. This often achieves higher performance than hard voting because it gives more weight to highly confident votes. Replace voting=\"hard\" with voting=\"soft\" and ensure all classifiers can estimate probabilities. For SVC, set probability=True (this uses cross-validation to estimate probabilities, slowing training). With soft voting, the classifier achieves over 91.2% accuracy. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bagging_pasting"
      },
      "source": [
        "**Bagging and Pasting**   \n",
        "Another approach to get diverse classifiers is using the same training algorithm for every predictor, training them on different random subsets of the training set. When sampling is performed **with replacement**, this method is called **bagging** (bootstrap aggregating). When sampling is performed **without replacement**, it's called **pasting**. \n",
        "\n",
        "**Figure 7-4. Bagging and pasting involves training several predictors on different random samples of the training set**   \n",
        "![Figure7-4.jpg](./07.Chapter-07/Figure7-4.jpg) \n",
        "\n",
        "Once all predictors are trained, the ensemble aggregates predictions using the statistical mode (most frequent prediction) for classification or average for regression. Each individual predictor has higher bias than if trained on the original set, but aggregation reduces both bias and variance. Generally, the ensemble has similar bias but lower variance than a single predictor. \n",
        "\n",
        "Predictors can all be trained in parallel via different CPU cores or servers, making bagging and pasting very scalable.\n",
        "\n",
        "**Bagging and Pasting in Scikit-Learn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bagging_code"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bagging_details"
      },
      "source": [
        "The code trains an ensemble of 500 Decision Tree classifiers, each on 100 training instances randomly sampled with replacement. The n_jobs parameter specifies CPU cores to use (-1 uses all available cores). For pasting instead of bagging, set bootstrap=False. \n",
        "\n",
        "**Note**: BaggingClassifier automatically performs soft voting if the base classifier can estimate class probabilities (has predict_proba() method). \n",
        "\n",
        "**Figure 7-5. A single Decision Tree (left) versus a bagging ensemble of 500 trees (right)**   \n",
        "![Figure7-5.jpg](./07.Chapter-07/Figure7-5.jpg) \n",
        "\n",
        "Bootstrapping introduces more diversity in subsets, so bagging has slightly higher bias than pasting, but the extra diversity reduces correlation between predictors, reducing variance. Overall, bagging often results in better models and is generally preferred.\n",
        "\n",
        "**Out-of-Bag Evaluation** \n",
        "With bagging, some instances may be sampled several times for a predictor, while others may not be sampled at all. By default, BaggingClassifier samples m training instances with replacement, meaning only about 63% are sampled on average for each predictor. The remaining 37% are called **out-of-bag (oob)** instances. \n",
        "\n",
        "Since a predictor never sees oob instances during training, it can be evaluated on them without needing a separate validation set. You can evaluate the ensemble by averaging oob evaluations of each predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oob_code"
      },
      "outputs": [],
      "source": [
        ">>> bag_clf = BaggingClassifier(\n",
        "...     DecisionTreeClassifier(), n_estimators=500,\n",
        "...     bootstrap=True, n_jobs=-1, oob_score=True)\n",
        "...\n",
        ">>> bag_clf.fit(X_train, y_train)\n",
        ">>> bag_clf.oob_score_\n",
        "0.90133333333333332"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oob_verify_text"
      },
      "source": [
        "This indicates the classifier will likely achieve about 90.1% accuracy on the test set. Verification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oob_verify_code"
      },
      "outputs": [],
      "source": [
        ">>> from sklearn.metrics import accuracy_score\n",
        ">>> y_pred = bag_clf.predict(X_test)\n",
        ">>> accuracy_score(y_test, y_pred)\n",
        "0.91200000000000003"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oob_function_text"
      },
      "source": [
        "The oob decision function for each training instance is available through oob_decision_function_. It returns class probabilities for each training instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oob_function_code"
      },
      "outputs": [],
      "source": [
        ">>> bag_clf.oob_decision_function_\n",
        "array([[0.31746032, 0.68253968],\n",
        "       [0.34117647, 0.65882353],\n",
        "       [1.        , 0.        ],\n",
        "       ...\n",
        "       [1.        , 0.        ],\n",
        "       [0.03108808, 0.96891192],\n",
        "       [0.57291667, 0.42708333]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "random_patches"
      },
      "source": [
        "**Random Patches and Random Subspaces**   \n",
        "BaggingClassifier supports sampling features as well. Sampling is controlled by max_features and bootstrap_features, which work like max_samples and bootstrap but for feature sampling. Each predictor trains on a random subset of input features. \n",
        "\n",
        "This is particularly useful for high-dimensional inputs like images. Sampling both training instances and features is called the **Random Patches method**. Keeping all training instances (bootstrap=False and max_samples=1.0) but sampling features (bootstrap_features=True and/or max_features < 1.0) is called the **Random Subspaces method**. \n",
        "\n",
        "Sampling features results in even more predictor diversity, trading more bias for lower variance. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "random_forests"
      },
      "source": [
        "**Random Forests**   \n",
        "A **Random Forest** is an ensemble of Decision Trees, generally trained via bagging (or sometimes pasting), typically with max_samples set to the training set size. Instead of using BaggingClassifier with DecisionTreeClassifier, use the more convenient and optimized RandomForestClassifier class (or RandomForestRegressor for regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf_code"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf_explanation"
      },
      "source": [
        "RandomForestClassifier has all hyperparameters of DecisionTreeClassifier (to control tree growth) plus all hyperparameters of BaggingClassifier (to control the ensemble). \n",
        "\n",
        "The Random Forest algorithm introduces extra randomness when growing trees: instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in greater tree diversity, trading higher bias for lower variance, generally yielding a better overall model. \n",
        "\n",
        "The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf_equivalent"
      },
      "outputs": [],
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
        "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extra_trees"
      },
      "source": [
        "**Extra-Trees**   \n",
        "When growing a tree in a Random Forest, at each node only a random subset of features is considered for splitting. It's possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds. \n",
        "\n",
        "A forest of such extremely random trees is called an **Extremely Randomized Trees ensemble** (Extra-Trees). This technique trades more bias for lower variance. It also makes Extra-Trees much faster to train than Random Forests because finding the best possible threshold is one of the most time-consuming tasks. \n",
        "\n",
        "Create an Extra-Trees classifier using ExtraTreesClassifier (or ExtraTreesRegressor). Its API is identical to RandomForestClassifier. \n",
        "\n",
        "**Note**: It's hard to tell in advance whether RandomForestClassifier or ExtraTreesClassifier will perform better. Generally, try both and compare using cross-validation with grid search.\n",
        "\n",
        "**Feature Importance**   \n",
        "Random Forests make it easy to measure the relative importance of each feature. Scikit-Learn measures feature importance by looking at how much tree nodes using that feature reduce impurity on average (across all trees in the forest). More precisely, it's a weighted average where each node's weight equals the number of training samples associated with it. \n",
        "\n",
        "Scikit-Learn computes this score automatically after training, scaling results so the sum of all importances equals 1. Access the result using feature_importances_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_importance"
      },
      "outputs": [],
      "source": [
        ">>> from sklearn.datasets import load_iris\n",
        ">>> iris = load_iris()\n",
        ">>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        ">>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        ">>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "...     print(name, score)\n",
        "...\n",
        "sepal length (cm) 0.112492250999\n",
        "sepal width (cm) 0.0231192882825\n",
        "petal length (cm) 0.441030464364\n",
        "petal width (cm) 0.423357996355"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_importance_text"
      },
      "source": [
        "The most important features are petal length (44%) and width (42%), while sepal length and width are relatively unimportant (11% and 2%). \n",
        "\n",
        "**Figure 7-6. MNIST pixel importance (according to a Random Forest classifier)**   \n",
        "![Figure7-6.jpg](./07.Chapter-07/Figure7-6.jpg) \n",
        "\n",
        "Random Forests are handy for quickly understanding what features matter, particularly for feature selection. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boosting_adaboost"
      },
      "source": [
        "**Boosting**   \n",
        "**Boosting** (originally hypothesis boosting) refers to any Ensemble method that combines several weak learners into a strong learner. The general idea is to train predictors sequentially, each trying to correct its predecessor. The most popular boosting methods are **AdaBoost** (Adaptive Boosting) and **Gradient Boosting**.\n",
        "\n",
        "**AdaBoost**   \n",
        "One way for a new predictor to correct its predecessor is to pay more attention to training instances that the predecessor underfitted. This results in new predictors focusing more on hard cases. This is the technique used by AdaBoost. \n",
        "\n",
        "When training an AdaBoost classifier, the algorithm first trains a base classifier (like a Decision Tree) and makes predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. It trains a second classifier using updated weights, makes predictions, updates weights, and so on. \n",
        "\n",
        "**Figure 7-7. AdaBoost sequential training with instance weight updates**   \n",
        "![Figure7-7.jpg](./07.Chapter-07/Figure7-7.jpg) \n",
        "\n",
        "**Figure 7-8. Decision boundaries of consecutive predictors**   \n",
        "![Figure7-8.jpg](./07.Chapter-07/Figure7-8.jpg) \n",
        "\n",
        "The figure shows decision boundaries of five consecutive predictors on the moons dataset (each predictor is a highly regularized SVM classifier with RBF kernel). The first classifier gets many instances wrong, so their weights get boosted. The second classifier does better on these instances, and so on. The right plot shows the same sequence with halved learning rate (misclassified instance weights boosted half as much at every iteration). \n",
        "\n",
        "Once all predictors are trained, the ensemble makes predictions like bagging or pasting, except predictors have different weights depending on their overall accuracy on the weighted training set. \n",
        "\n",
        "**Important drawback**: This sequential learning technique cannot be parallelized (or only partially) since each predictor can only be trained after the previous predictor has been trained and evaluated. It doesn't scale as well as bagging or pasting.\n",
        "\n",
        "**AdaBoost Algorithm Details**   \n",
        "Each instance weight w<sup>(i)</sup> is initially set to 1/m. A first predictor is trained, and its weighted error rate r1 is computed on the training set. \n",
        "\n",
        "**Equation 7-1. Weighted error rate of the jth predictor**   \n",
        "![Eq7-1.jpg](./07.Chapter-07/Eq7-1.jpg) \n",
        "\n",
        "Where ŷ<sub>j</sub><sup>(i)</sup> is the jth predictor's prediction for the ith instance. \n",
        "\n",
        "The predictor's weight αj is then computed using Equation 7-2, where η is the learning rate hyperparameter (defaults to 1). The more accurate the predictor, the higher its weight. If it's just guessing randomly, its weight is close to zero. If it's most often wrong (less accurate than random guessing), its weight is negative. \n",
        "\n",
        "**Equation 7-2. Predictor weight**   \n",
        "![Eq7-2.jpg](./07.Chapter-07/Eq7-2.jpg) \n",
        "\n",
        "Next, AdaBoost updates instance weights using Equation 7-3, which boosts weights of misclassified instances. \n",
        "\n",
        "**Equation 7-3. Weight update rule**   \n",
        "![Eq7-3.jpg](./07.Chapter-07/Eq7-3.jpg) \n",
        "\n",
        "Then all instance weights are normalized (divided by Σw(i)). \n",
        "\n",
        "Finally, a new predictor is trained using updated weights, and the process repeats. The algorithm stops when the desired number of predictors is reached or when a perfect predictor is found. \n",
        "\n",
        "To make predictions, AdaBoost computes predictions of all predictors and weighs them using predictor weights αj. The predicted class receives the majority of weighted votes. \n",
        "\n",
        "**Equation 7-4. AdaBoost predictions**   \n",
        "![Eq7-4.jpg](./07.Chapter-07/Eq7-4.jpg) \n",
        "\n",
        "Where N is the number of predictors.\n",
        "\n",
        "**Scikit-Learn Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adaboost_code"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
        "ada_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gradient_boosting"
      },
      "source": [
        "The code trains an AdaBoost classifier based on 200 Decision Stumps. A **Decision Stump** is a Decision Tree with max_depth=1—a tree with a single decision node plus two leaf nodes. This is the default base estimator for AdaBoostClassifier. \n",
        "\n",
        "Scikit-Learn uses a multiclass version of AdaBoost called **SAMME** (Stagewise Additive Modeling using a Multiclass Exponential loss function). When there are just two classes, SAMME is equivalent to AdaBoost. If predictors can estimate class probabilities (have predict_proba() method), Scikit-Learn uses **SAMME.R** (R stands for \"Real\"), which relies on class probabilities rather than predictions and generally performs better. \n",
        "\n",
        "**Note**: If your AdaBoost ensemble is overfitting, try reducing the number of estimators or more strongly regularizing the base estimator.\n",
        "\n",
        "**Gradient Boosting**   \n",
        "Another popular boosting algorithm is **Gradient Boosting**. Like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each correcting its predecessor. However, instead of tweaking instance weights at every iteration like AdaBoost, this method tries to fit the new predictor to the residual errors made by the previous predictor. \n",
        "\n",
        "**Simple Regression Example**   \n",
        "Using Decision Trees as base predictors (Gradient Boosting works great with regression tasks), this is called **Gradient Tree Boosting** or **Gradient Boosted Regression Trees (GBRT)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbrt_step1"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg1.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrt_step2_text"
      },
      "source": [
        "Next, train a second DecisionTreeRegressor on the residual errors made by the first predictor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbrt_step2"
      },
      "outputs": [],
      "source": [
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg2.fit(X, y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrt_step3_text"
      },
      "source": [
        "Then train a third regressor on the residual errors made by the second predictor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbrt_step3"
      },
      "outputs": [],
      "source": [
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg3.fit(X, y3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrt_predict_text"
      },
      "source": [
        "Now the ensemble containing three trees can make predictions by adding up predictions of all trees:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbrt_predict"
      },
      "outputs": [],
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrt_figure"
      },
      "source": [
        "**Figure 7-9. Gradient Boosting: the first predictor (top left) is trained normally, then each consecutive predictor (middle left and lower left) is trained on the previous predictor's residuals; the right column shows the resulting ensemble's predictions**   \n",
        "![Figure7-9.jpg](./07.Chapter-07/Figure7-9.jpg) \n",
        "\n",
        "**Using GradientBoostingRegressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbrt_sklearn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
        "gbrt.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbrt_learning_rate"
      },
      "source": [
        "The code creates the same ensemble as the previous one. The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value like 0.1, you'll need more trees in the ensemble to fit the training set, but predictions will usually generalize better. This is a regularization technique called **shrinkage**. \n",
        "\n",
        "**Figure 7-10. GBRT ensembles with not enough predictors (left) and too many (right)**   \n",
        "![Figure7-10.jpg](./07.Chapter-07/Figure7-10.jpg) \n",
        "\n",
        "**Early Stopping**   \n",
        "To find the optimal number of trees, use early stopping. A simple way is using the staged_predict() method: it returns an iterator over predictions made by the ensemble at each training stage (with one tree, two trees, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "early_stopping"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
        "gbrt_best.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "early_stopping_figure"
      },
      "source": [
        "**Figure 7-11. Tuning the number of trees using early stopping**   \n",
        "![Figure7-11.jpg](./07.Chapter-07/Figure7-11.jpg) \n",
        "\n",
        "It's also possible to implement early stopping by actually stopping training early (instead of training many trees first and looking back). Set warm_start=True to make Scikit-Learn keep existing trees when fit() is called, allowing incremental training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "warm_start"
      },
      "outputs": [],
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    y_pred = gbrt.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break  # early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stochastic_xgboost"
      },
      "source": [
        "The code stops training when validation error doesn't improve for five iterations in a row. \n",
        "\n",
        "**Stochastic Gradient Boosting**   \n",
        "GradientBoostingRegressor also supports a subsample hyperparameter, specifying the fraction of training instances used for training each tree. For example, if subsample=0.25, each tree is trained on 25% of training instances, selected randomly. This technique trades higher bias for lower variance and speeds up training considerably. This is called **Stochastic Gradient Boosting**. \n",
        "\n",
        "**Note**: It's possible to use Gradient Boosting with other cost functions controlled by the loss hyperparameter.\n",
        "\n",
        "**XGBoost**   \n",
        "An optimized implementation of Gradient Boosting is available in the popular Python library **XGBoost** (Extreme Gradient Boosting). Initially developed by Tianqi Chen as part of the Distributed (Deep) Machine Learning Community (DMLC), it aims to be extremely fast, scalable, and portable. XGBoost is often an important component of winning entries in ML competitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgboost_basic"
      },
      "outputs": [],
      "source": [
        "import xgboost\n",
        "\n",
        "xgb_reg = xgboost.XGBRegressor()\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgboost_early_text"
      },
      "source": [
        "XGBoost's API is quite similar to Scikit-Learn's. XGBoost offers several nice features, such as automatically taking care of early stopping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgboost_early"
      },
      "outputs": [],
      "source": [
        "xgb_reg.fit(X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
        "y_pred = xgb_reg.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "separator"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stacking"
      },
      "source": [
        "**Stacking**   \n",
        "The last Ensemble method discussed is **stacking** (short for stacked generalization). It's based on a simple idea: instead of using trivial functions (like hard voting) to aggregate predictions of all predictors in an ensemble, why not train a model to perform this aggregation? \n",
        "\n",
        "**Figure 7-12. Aggregating predictions using a blending predictor**   \n",
        "![Figure7-12.jpg](./07.Chapter-07/Figure7-12.jpg) \n",
        "\n",
        "The figure shows an ensemble performing a regression task on a new instance. Each of the bottom three predictors predicts a different value (3.1, 2.7, and 2.9), and the final predictor (called a **blender** or **meta learner**) takes these predictions as inputs and makes the final prediction (3.0).\n",
        "\n",
        "**Training the Blender**   \n",
        "To train the blender, a common approach is using a **hold-out set**. First, the training set is split into two subsets. The first subset is used to train the predictors in the first layer. \n",
        "\n",
        "**Figure 7-13. Training the first layer**   \n",
        "![Figure7-13.jpg](./07.Chapter-07/Figure7-13.jpg) \n",
        "\n",
        "Next, the first layer's predictors make predictions on the second (held-out) set. This ensures predictions are \"clean,\" since predictors never saw these instances during training. For each instance in the hold-out set, there are three predicted values. Create a new training set using these predicted values as input features (making this new training set 3D), keeping the target values. The blender is trained on this new training set, learning to predict the target value given the first layer's predictions. \n",
        "\n",
        "**Figure 7-14. Training the blender**   \n",
        "![Figure7-14.jpg](./07.Chapter-07/Figure7-14.jpg) \n",
        "\n",
        "**Multilayer Stacking**   \n",
        "It's possible to train several different blenders this way (e.g., one using Linear Regression, another using Random Forest Regression) to get a whole layer of blenders. The trick is to split the training set into three subsets: the first trains the first layer, the second creates the training set for the second layer (using first layer predictions), and the third creates the training set for the third layer (using second layer predictions). Once done, make a prediction for a new instance by going through each layer sequentially. \n",
        "\n",
        "**Figure 7-15. Predictions in a multilayer stacking ensemble**   \n",
        "![Figure7-15.jpg](./07.Chapter-07/Figure7-15.jpg) \n",
        "\n",
        "Unfortunately, Scikit-Learn doesn't support stacking directly, but it's not too hard to implement yourself. Alternatively, use an open source implementation such as DESlib."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
