{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "author_cell"
      },
      "source": [
        "**AUTHOR: RAIHAN SALMAN BAEHAQI (1103220180)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part_header"
      },
      "source": [
        "**PART I** \n",
        "\n",
        "**The Fundamentals of Machine Learning** \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chapter_header"
      },
      "source": [
        "**CHAPTER 6 - Decision Trees** \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "Chapter 6 explores Decision Trees, versatile Machine Learning algorithms capable of performing classification, regression, and multioutput tasks. They are powerful algorithms capable of fitting complex datasets and serve as fundamental components of Random Forests, which are among the most powerful ML algorithms available today.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_viz"
      },
      "source": [
        "**Training and Visualizing a Decision Tree**   \n",
        "To understand Decision Trees, let's build one using the iris dataset. The following code trains a DecisionTreeClassifier on the iris dataset using petal length and width features.\n",
        "\n",
        "Train a Decision Tree classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_tree"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:]  # petal length and width\n",
        "y = iris.target\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
        "tree_clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viz_graphviz"
      },
      "source": [
        "**Visualizing with Graphviz**   \n",
        "You can visualize the trained Decision Tree using the export_graphviz() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_viz"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(\n",
        "    tree_clf,\n",
        "    out_file=\"iris_tree.dot\",\n",
        "    feature_names=iris.feature_names[2:],\n",
        "    class_names=iris.target_names,\n",
        "    rounded=True,\n",
        "    filled=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convert_dot"
      },
      "source": [
        "Convert the DOT file to PNG using Graphviz command-line tool:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dot_command"
      },
      "outputs": [],
      "source": [
        "# In terminal/command line:\n",
        "# $ dot -Tpng iris_tree.dot -o iris_tree.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figure_6_1"
      },
      "source": [
        "**Figure 6-1. Iris Decision Tree**   \n",
        "![Figure6-1.jpg](./06.Chapter-06/Figure6-1.jpg)\n",
        "\n",
        "The tree shows nodes with gini impurity, samples count, value arrays, and class predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "making_predictions"
      },
      "source": [
        "**Making Predictions**   \n",
        "Decision Trees make predictions by traversing from the root node to leaf nodes based on feature thresholds. Starting at the root node (depth 0), the tree checks if petal length < 2.45 cm. If true, it predicts Iris setosa. Otherwise, it checks if petal width < 1.75 cm to predict either Iris versicolor or Iris virginica.\n",
        "\n",
        "**Data Preparation Requirements**   \n",
        "One quality of Decision Trees is that they require very little data preparation. They don't require feature scaling or centering at all.\n",
        "\n",
        "**Node Attributes:**  \n",
        "- **samples**: counts how many training instances apply to the node\n",
        "- **value**: number of training instances of each class at the node\n",
        "- **gini**: measures node impurity (gini=0 means pure node)\n",
        "\n",
        "**Equation 6-1. Gini impurity**   \n",
        "![Eq6-1.jpg](./06.Chapter-06/Eq6-1.jpg)\n",
        "\n",
        "Where p_i,k is the ratio of class k instances among training instances in the ith node.\n",
        "\n",
        "**Binary Trees in Scikit-Learn**   \n",
        "Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children.\n",
        "\n",
        "**Figure 6-2. Decision Tree decision boundaries**     \n",
        "![Figure6-2.jpg](./06.Chapter-06/Figure6-2.jpg)\n",
        "\n",
        "The thick vertical line represents the root node decision boundary (petal length = 2.45 cm). The dashed line shows the depth-1 right node split (petal width = 1.75 cm).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "white_box"
      },
      "source": [
        "**Model Interpretation: White Box Versus Black Box**   \n",
        "Decision Trees are intuitive and their decisions are easy to interpret. Such models are called **white box models**. In contrast, Random Forests or neural networks are **black box models**: they make great predictions but it's hard to explain in simple terms why the predictions were made. Decision Trees provide simple classification rules that can even be applied manually.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "class_prob"
      },
      "source": [
        "**Estimating Class Probabilities**   \n",
        "A Decision Tree can estimate the probability that an instance belongs to a particular class k. It traverses the tree to find the leaf node for the instance, then returns the ratio of training instances of class k in that node.\n",
        "\n",
        "Example prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "predict_proba"
      },
      "outputs": [],
      "source": [
        ">>> tree_clf.predict_proba([[5, 1.5]])\n",
        "array([[0.        , 0.90740741, 0.09259259]])\n",
        ">>> tree_clf.predict([[5, 1.5]])\n",
        "array([1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prob_explanation"
      },
      "source": [
        "For a flower with petals 5 cm long and 1.5 cm wide, the tree predicts 0% for Iris setosa, 90.7% for Iris versicolor, and 9.3% for Iris virginica. The predicted class is Iris versicolor (class 1).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cart_algorithm"
      },
      "source": [
        "**The CART Training Algorithm**     \n",
        "Scikit-Learn uses the **Classification and Regression Tree (CART)** algorithm to train Decision Trees. The algorithm splits the training set into two subsets using a single feature k and threshold t_k that produces the purest subsets (weighted by their size).\n",
        "\n",
        "**Equation 6-2. CART cost function for classification**   \n",
        "![Eq6-2.jpg](./06.Chapter-06/Eq6-2.jpg)\n",
        "\n",
        "Where G<sub>left/right</sub> measures the impurity of the left/right subset, and m<sub>left/right</sub> is the number of instances in each subset.\n",
        "\n",
        "The algorithm recursively splits subsets until reaching maximum depth (max_depth) or when no split reduces impurity. Other stopping conditions include: min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes.\n",
        "\n",
        "**Greedy Algorithm**   \n",
        "CART is a **greedy algorithm**: it searches for an optimum split at the top level, then repeats at each subsequent level. It doesn't guarantee a globally optimal tree. Finding the optimal tree is an **NP-Complete problem** requiring O(exp(m)) time.\n",
        "\n",
        "**Computational Complexity:**  \n",
        "- **Prediction**: O(log₂(m)) - requires traversing roughly log₂(m) nodes\n",
        "- **Training**: O(n × m log₂(m)) - compares all features on all samples at each node\n",
        "\n",
        "For small training sets (<few thousand instances), setting presort=True speeds up training, but slows it down for larger sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gini_entropy"
      },
      "source": [
        "**Gini Impurity or Entropy?**   \n",
        "By default, Gini impurity is used, but you can select entropy by setting criterion=\"entropy\". Entropy originated in thermodynamics and information theory, measuring average information content.\n",
        "\n",
        "**Equation 6-3. Entropy**   \n",
        "![Eq6-3.jpg](./06.Chapter-06/Eq6-3.jpg)\n",
        "\n",
        "Most of the time, Gini impurity and entropy lead to similar trees. Gini impurity is slightly faster to compute (good default), while entropy tends to produce slightly more balanced trees.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regularization"
      },
      "source": [
        "**Regularization Hyperparameters**   \n",
        "Decision Trees are **nonparametric models** - the number of parameters is not determined prior to training, allowing the model structure to stick closely to the data. If left unconstrained, the tree will likely overfit.\n",
        "\n",
        "**Main Regularization Parameters:**  \n",
        "- **max_depth**: maximum tree depth (default is None/unlimited)\n",
        "- **min_samples_split**: minimum samples needed before splitting a node\n",
        "- **min_samples_leaf**: minimum samples required in leaf nodes\n",
        "- **min_weight_fraction_leaf**: same as min_samples_leaf but as a fraction\n",
        "- **max_leaf_nodes**: maximum number of leaf nodes\n",
        "- **max_features**: maximum features evaluated for splitting at each node\n",
        "\n",
        "Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.\n",
        "\n",
        "**Pruning**   \n",
        "Alternative algorithms first train the tree without restrictions, then prune (delete) unnecessary nodes. A node is considered unnecessary if the purity improvement it provides is not statistically significant. Standard tests like the **chi-squared test** estimate if improvement is due to chance. If the p-value is higher than a threshold (typically 5%), the node is deleted.\n",
        "\n",
        "**Figure 6-3. Regularization using min_samples_leaf**   \n",
        "![Figure6-3.jpg](./06.Chapter-06/Figure6-3.jpg)\n",
        "\n",
        "Left: Decision Tree with default hyperparameters (overfitting). Right: trained with min_samples_leaf=4 (better generalization).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regression"
      },
      "source": [
        "**Regression**   \n",
        "Decision Trees can perform regression tasks using DecisionTreeRegressor class.\n",
        "\n",
        "Train a regression tree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_regressor"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regression_figures"
      },
      "source": [
        "**Figure 6-4. A Decision Tree for regression**   \n",
        "![Figure6-4.jpg](./06.Chapter-06/Figure6-4.jpg)\n",
        "\n",
        "Instead of predicting a class, the regression tree predicts a value. For instance with x1 = 0.6, it predicts value=0.111, which is the average target value of the 110 training instances in that leaf node.\n",
        "\n",
        "**Figure 6-5. Predictions of two Decision Tree regression models**   \n",
        "![Figure6-5.jpg](./06.Chapter-06/Figure6-5.jpg)\n",
        "\n",
        "Left: max_depth=2. Right: max_depth=3. The predicted value for each region is always the average target value of instances in that region.\n",
        "\n",
        "**CART Cost Function for Regression**  \n",
        "\n",
        "**Equation 6-4. CART cost function for regression**   \n",
        "![Eq6-4.jpg](./06.Chapter-06/Eq6-4.jpg)\n",
        "\n",
        "The CART algorithm tries to split the training set to minimize the MSE instead of minimizing impurity.\n",
        "\n",
        "**Regularizing Regression Trees**   \n",
        "Decision Trees are prone to overfitting when dealing with regression tasks. Without regularization, predictions overfit badly. Setting min_samples_leaf=10 results in a much more reasonable model.\n",
        "\n",
        "**Figure 6-6. Regularizing a Decision Tree regressor**   \n",
        "![Figure6-6.jpg](./06.Chapter-06/Figure6-6.jpg)\n",
        "\n",
        "Left: overfitting without regularization. Right: reasonable predictions with min_samples_leaf=10.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instability"
      },
      "source": [
        "**Instability**   \n",
        "Decision Trees are simple to understand, easy to use, versatile, and powerful. However, they do have limitations.\n",
        "\n",
        "**Sensitivity to Training Set Rotation**   \n",
        "Decision Trees love **orthogonal decision boundaries** (all splits perpendicular to an axis), making them sensitive to training set rotation.\n",
        "\n",
        "**Figure 6-7. Sensitivity to training set rotation**   \n",
        "![Figure6-7.jpg](./06.Chapter-06/Figure6-7.jpg)\n",
        "\n",
        "Left: Decision Tree splits linearly separable dataset easily. Right: after 45° rotation, the decision boundary looks unnecessarily convoluted. One solution is to use Principal Component Analysis (Chapter 8) for better orientation of training data.\n",
        "\n",
        "**Sensitivity to Small Variations**   \n",
        "The main issue with Decision Trees is high sensitivity to small variations in training data. Removing just one training instance can produce a very different model.\n",
        "\n",
        "**Figure 6-8. Sensitivity to training set details**   \n",
        "![Figure6-8.jpg](./06.Chapter-06/Figure6-8.jpg)\n",
        "\n",
        "After removing one training instance, the Decision Tree looks very different. Since Scikit-Learn's training algorithm is **stochastic** (randomly selects features to evaluate at each node), you may get very different models even on the same training data (unless you set the random_state hyperparameter)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
