{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AUTHOR: RAIHAN SALMAN BAEHAQI (1103220180)**"
      ],
      "metadata": {
        "id": "W2TDcpsZNupH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART I**  \n",
        "\n",
        "**The Fundamentals of Machine Learning**  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xIq5hI6qOO2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHAPTER 3 - Classification**  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oiuzyZ0-OSBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 3 shifts focus from regression to classification tasks, using the MNIST handwritten digit dataset to explore binary and multiclass classification systems, performance evaluation metrics, and error analysis techniques  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "uJe2nPU4OXtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MNIST**  \n",
        "\n",
        "The MNIST dataset consists of 70,000 small images of handwritten digits (0-9) by high school students and US Census Bureau employees, known as the \"hello world\" of Machine Learning. Each image is labeled with its corresponding digit.  \n",
        "\n",
        "**Loading the Dataset**  \n",
        "Scikit-Learn provides helper functions to download popular datasets:  "
      ],
      "metadata": {
        "id": "vcxEmjYkOnJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.datasets import fetch_openml\n",
        ">>> mnist = fetch_openml('mnist_784', version=1)\n",
        ">>> mnist.keys()\n",
        "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',\n",
        "           'categories', 'url'])"
      ],
      "metadata": {
        "id": "8TpBhyfrO2nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn datasets have a standard dictionary structure with DESCR (dataset description), data (feature array), and target (label array) keys.  \n",
        "\n",
        "**Exploring the Data**  \n",
        "Examine the arrays:"
      ],
      "metadata": {
        "id": "_LabTitEO_nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> X, y = mnist[\"data\"], mnist[\"target\"]\n",
        ">>> X.shape\n",
        "(70000, 784)\n",
        ">>> y.shape\n",
        "(70000,)"
      ],
      "metadata": {
        "id": "JCoVX9BKPFgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 70,000 images with 784 features each (28 × 28 pixels), where each feature represents pixel intensity from 0 (white) to 255 (black).\n",
        "\n",
        "Display a digit image:"
      ],
      "metadata": {
        "id": "hiyAEeWrPK4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "some_digit = X[0]\n",
        "some_digit_image = some_digit.reshape(28, 28)\n",
        "plt.imshow(some_digit_image, cmap=\"binary\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YUUqCP_8POpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the label:"
      ],
      "metadata": {
        "id": "CcL2ikzDPQXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> y[0]\n",
        "'5'"
      ],
      "metadata": {
        "id": "rpufbtPtPSfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert labels from strings to integers:"
      ],
      "metadata": {
        "id": "EMtMvw6qPUlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> y = y.astype(np.uint8)"
      ],
      "metadata": {
        "id": "wuurBHaJPV_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure3-1.jpg](./03.Chapter-03/Figure3-1.jpg)  \n",
        "\n",
        "**Creating Train/Test Sets**  \n",
        "\n",
        "The MNIST dataset is pre-split into training (first 60,000 images) and test (last 10,000 images) sets:"
      ],
      "metadata": {
        "id": "7DPuCVsNPYZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ],
      "metadata": {
        "id": "titiKD_1PpFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set is already shuffled, ensuring similar cross-validation folds and preventing poor performance from learning algorithms sensitive to instance order. Shuffling may be inappropriate for time series data.​  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GtjAmDnIPqYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training a Binary Classifier**  \n",
        "\n",
        "Simplify the problem by creating a \"5-detector\" binary classifier distinguishing between 5 and not-5:"
      ],
      "metadata": {
        "id": "TAcLpVedPud2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_5 = (y_train == 5)  # True for all 5s, False for all other digits\n",
        "y_test_5 = (y_test == 5)"
      ],
      "metadata": {
        "id": "yHgnAcWQPzPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Stochastic Gradient Descent (SGD) classifier:"
      ],
      "metadata": {
        "id": "Lv0F5Tw_TKQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(random_state=42)\n",
        "sgd_clf.fit(X_train, y_train_5)"
      ],
      "metadata": {
        "id": "GZ2lthOXTOkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SGDClassifier handles very large datasets efficiently by processing training instances independently, making it suitable for online learning. Setting random_state ensures reproducible results.​\n",
        "\n",
        "Make predictions:"
      ],
      "metadata": {
        "id": "FoJFQAO9TRUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> sgd_clf.predict([some_digit])\n",
        "array([ True])"
      ],
      "metadata": {
        "id": "iSw6pvVgTUbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier correctly identifies the digit 5.  \n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Tw-ZKNjgTWCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Performance Measures**  \n",
        "\n",
        "Evaluating classifiers is significantly trickier than evaluating regressors, requiring careful metric selection."
      ],
      "metadata": {
        "id": "AlzrfvYlTYP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measuring Accuracy Using Cross-Validation**  \n",
        "\n",
        "**Implementing Cross-Validation**  \n",
        "Manual cross-validation implementation for greater control:"
      ],
      "metadata": {
        "id": "q1CiUK2iTkda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.base import clone\n",
        "\n",
        "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
        "\n",
        "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
        "    clone_clf = clone(sgd_clf)\n",
        "    X_train_folds = X_train[train_index]\n",
        "    y_train_folds = y_train_5[train_index]\n",
        "    X_test_fold = X_train[test_index]\n",
        "    y_test_fold = y_train_5[test_index]\n",
        "\n",
        "    clone_clf.fit(X_train_folds, y_train_folds)\n",
        "    y_pred = clone_clf.predict(X_test_fold)\n",
        "    n_correct = sum(y_pred == y_test_fold)\n",
        "    print(n_correct / len(y_pred))  # prints 0.9502, 0.96565, and 0.96495"
      ],
      "metadata": {
        "id": "JfRp7AM2TuTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StratifiedKFold performs stratified sampling to produce folds with representative class ratios.​\n",
        "\n",
        "Use cross_val_score for simpler evaluation:"
      ],
      "metadata": {
        "id": "7h8sg_KrTyPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.model_selection import cross_val_score\n",
        ">>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
        "array([0.96355, 0.93795, 0.95615])"
      ],
      "metadata": {
        "id": "kJDRDalXT1el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over 93% accuracy appears impressive, but consider a \"dumb\" classifier:"
      ],
      "metadata": {
        "id": "4cvmFsFmT39x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class Never5Classifier(BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        return np.zeros((len(X), 1), dtype=bool)\n",
        "\n",
        ">>> never_5_clf = Never5Classifier()\n",
        ">>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
        "array([0.91125, 0.90855, 0.90915])"
      ],
      "metadata": {
        "id": "kj7OYatFT5gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dummy classifier achieves over 90% accuracy simply because only 10% of images are 5s. This demonstrates why accuracy is generally not preferred for classifiers, especially with skewed datasets where some classes are much more frequent."
      ],
      "metadata": {
        "id": "RSc3d4KdT68W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**  \n",
        "A confusion matrix counts how many times instances of class A are classified as class B.​\n",
        "\n",
        "Generate predictions using cross_val_predict:"
      ],
      "metadata": {
        "id": "JvOxGqc9UH3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
      ],
      "metadata": {
        "id": "yTdbSsXIUOiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike cross_val_score, cross_val_predict returns predictions made on each test fold using a model that never saw the data during training.  \n",
        "\n",
        "Compute the confusion matrix:"
      ],
      "metadata": {
        "id": "zr5DdX4KUQRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.metrics import confusion_matrix\n",
        ">>> confusion_matrix(y_train_5, y_train_pred)\n",
        "array([[53057,  1522],\n",
        "       [ 1325,  4096]])"
      ],
      "metadata": {
        "id": "kd2ZcGx7UUBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row represents an actual class, each column represents a predicted class:​\n",
        "* True negatives (TN): 53,057 non-5s correctly classified as non-5s​\n",
        "* False positives (FP): 1,522 non-5s wrongly classified as 5s​\n",
        "* False negatives (FN): 1,325 5s wrongly classified as non-5s​\n",
        "* True positives (TP): 4,096 5s correctly classified as 5s​\n",
        "\n",
        "A perfect classifier has only true positives and true negatives (nonzero values only on the main diagonal):"
      ],
      "metadata": {
        "id": "k96Y9ijhUrRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> y_train_perfect_predictions = y_train_5\n",
        ">>> confusion_matrix(y_train_5, y_train_perfect_predictions)\n",
        "array([[54579,     0],\n",
        "       [    0,  5421]])"
      ],
      "metadata": {
        "id": "d_fM61sSUzOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure3-2.jpg](./03.Chapter-03/Figure3-2.jpg)"
      ],
      "metadata": {
        "id": "5_R86uYaU1L5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision and Recall**  \n",
        "\n",
        "Precision measures the accuracy of positive predictions:  \n",
        "\n",
        "Equation 3-1. Precision  \n",
        "![Eq3-1.jpg](./03.Chapter-03/Eq3-1.jpg)  \n",
        "\n",
        "where TP is true positives and FP is false positives.​\n",
        "\n",
        "Recall (sensitivity or true positive rate) measures the ratio of positive instances correctly detected:  \n",
        "\n",
        "Equation 3-2. Recall  \n",
        "![Eq3-2.jpg](./03.Chapter-03/Eq3-2.jpg)   \n",
        "\n",
        "where FN is false negatives.​\n",
        "\n",
        "Compute precision and recall:\n"
      ],
      "metadata": {
        "id": "7XgkvLn5U7Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.metrics import precision_score, recall_score\n",
        ">>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)\n",
        "0.7290850836596654\n",
        ">>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)\n",
        "0.7555801512636044"
      ],
      "metadata": {
        "id": "LN1PgFwCVcs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 5-detector is correct only 72.9% when it claims an image represents a 5, and detects only 75.6% of all 5s.  \n",
        "\n",
        "F1 Score\n",
        "The F1 score combines precision and recall into a single metric using the harmonic mean:  \n",
        "\n",
        "Equation 3-3. F1  \n",
        "![Eq3-3.jpg](./03.Chapter-03/Eq3-3.jpg)  \n",
        "\n",
        "The harmonic mean gives much more weight to low values, so the classifier only achieves high F1 scores when both precision and recall are high."
      ],
      "metadata": {
        "id": "wdggMRjDVewB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.metrics import f1_score\n",
        ">>> f1_score(y_train_5, y_train_pred)\n",
        "0.7420962043663375"
      ],
      "metadata": {
        "id": "eTkAWdnRVwcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F1 score favors classifiers with similar precision and recall. However, different contexts prioritize different metrics: a kid-safe video classifier should favor high precision (rejecting many good videos but keeping only safe ones), while a shoplifter detector should favor high recall (30% precision acceptable with 99% recall).  "
      ],
      "metadata": {
        "id": "N_gd-TVLVxpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision/Recall Trade-off**  \n",
        "Increasing precision reduces recall, and vice versa.​\n",
        "\n",
        "The SGDClassifier computes a score based on a decision function and assigns instances to the positive class if the score exceeds a threshold.  \n",
        "\n",
        "![Figure3-3.jpg](./03.Chapter-03/Figure3-3.jpg)  \n",
        "\n",
        "Access decision scores directly:"
      ],
      "metadata": {
        "id": "Tt2MjJDXV2Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> y_scores = sgd_clf.decision_function([some_digit])\n",
        ">>> y_scores\n",
        "array([2412.53175101])\n",
        ">>> threshold = 0\n",
        ">>> y_some_digit_pred = (y_scores > threshold)\n",
        "array([ True])"
      ],
      "metadata": {
        "id": "Br-YRYVKWB3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SGDClassifier uses threshold = 0 by default. Raising the threshold decreases recall:"
      ],
      "metadata": {
        "id": "0JajUmv_WD9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> threshold = 8000\n",
        ">>> y_some_digit_pred = (y_scores > threshold)\n",
        ">>> y_some_digit_pred\n",
        "array([False])"
      ],
      "metadata": {
        "id": "1cFTqJKfWGPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting the Right Threshold**  \n",
        "Get scores for all training instances:"
      ],
      "metadata": {
        "id": "zn3dw02cWH5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
        "                             method=\"decision_function\")"
      ],
      "metadata": {
        "id": "bQXhk5XgWPWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute precision and recall for all possible thresholds:"
      ],
      "metadata": {
        "id": "zpmeNiufWRqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
      ],
      "metadata": {
        "id": "eo71V3u3WThW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot precision and recall versus threshold:"
      ],
      "metadata": {
        "id": "kJt0aEAGWVB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
        "    # [...] highlight threshold, add legend, axis labels, and grid\n",
        "\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nb71Lb3AWXj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure3-4.jpg](./03.Chapter-03/Figure3-4.jpg)  \n",
        "\n",
        "Plot precision directly against recall:  \n",
        "\n",
        "![Figure3-5.jpg](./03.Chapter-03/Figure3-5.jpg)  \n",
        "\n",
        "To achieve 90% precision:"
      ],
      "metadata": {
        "id": "RHMcooSfWa7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816\n",
        "\n",
        "y_train_pred_90 = (y_scores >= threshold_90_precision)\n",
        "\n",
        ">>> precision_score(y_train_5, y_train_pred_90)\n",
        "0.9000380083618396\n",
        ">>> recall_score(y_train_5, y_train_pred_90)\n",
        "0.4368197749492714"
      ],
      "metadata": {
        "id": "d8CCtOJKWvtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a 90% precision classifier is easy by setting a high threshold, but this results in only 43.7% recall. When someone requests \"99% precision,\" always ask \"at what recall?\"."
      ],
      "metadata": {
        "id": "GDdABqgDWyS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The ROC Curve**  \n",
        "The receiver operating characteristic (ROC) curve plots the true positive rate (recall) against the false positive rate (FPR). FPR is the ratio of negative instances incorrectly classified as positive, equal to 1 - TNR (true negative rate or specificity).  \n",
        "\n",
        "Compute TPR and FPR:"
      ],
      "metadata": {
        "id": "dxao10QaWzlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
      ],
      "metadata": {
        "id": "Drsz7t9VXMoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the ROC curve:"
      ],
      "metadata": {
        "id": "HDwoOayfXOmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(fpr, tpr, label=None):\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
        "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n",
        "    # [...] Add axis labels and grid\n",
        "\n",
        "plot_roc_curve(fpr, tpr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OhzfPQ-dXQW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure3-6.jpg](./03.Chapter-03/Figure3-6.jpg)  \n",
        "\n",
        "Measure the area under the curve (AUC):"
      ],
      "metadata": {
        "id": "KjpoKDfGXSBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.metrics import roc_auc_score\n",
        ">>> roc_auc_score(y_train_5, y_scores)\n",
        "0.9611778893101814"
      ],
      "metadata": {
        "id": "_j910SpbXb3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A perfect classifier has ROC AUC = 1, while a purely random classifier has ROC AUC = 0.5.​\n",
        "\n",
        "**Rule of thumb:** Prefer the precision/recall (PR) curve when the positive class is rare or when you care more about false positives than false negatives; otherwise use the ROC curve. The ROC curve may appear overly optimistic with imbalanced datasets, while the PR curve makes improvement opportunities clearer.​\n",
        "\n",
        "**Comparing Classifiers: Random Forest**  \n",
        "Train a RandomForestClassifier and compare with SGDClassifier:"
      ],
      "metadata": {
        "id": "INtA0QfCXdbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest_clf = RandomForestClassifier(random_state=42)\n",
        "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
        "                                    method=\"predict_proba\")"
      ],
      "metadata": {
        "id": "XKFa7ffeXm_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestClassifier has predict_proba() instead of decision_function(), returning probabilities for each class.​\n",
        "\n",
        "Use the positive class probability as score:"
      ],
      "metadata": {
        "id": "YeDDssP3Xq5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_scores_forest = y_probas_forest[:, 1]   # score = proba of positive class\n",
        "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)"
      ],
      "metadata": {
        "id": "inn-vxV1Xs0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot both ROC curves:"
      ],
      "metadata": {
        "id": "PFoPYqi-XuSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
        "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rbt0g4vjXv5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure3-7.jpg](./03.Chapter-03/Figure3-7.jpg)"
      ],
      "metadata": {
        "id": "LnGCH9C7XxY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> roc_auc_score(y_train_5, y_scores_forest)\n",
        "0.9983436731328145"
      ],
      "metadata": {
        "id": "FK5qozhmX1Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest achieves significantly better ROC AUC (0.998 vs 0.961), with 99.0% precision and 86.6% recall.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LJ2y3klnX22j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiclass Classification**  \n",
        "\n",
        "Multiclass classifiers (multinomial classifiers) distinguish between more than two classes.​\n",
        "\n",
        "Some algorithms (SGD, Random Forest, naive Bayes) handle multiple classes natively. Others (Logistic Regression, Support Vector Machine) are strictly binary but can use strategies for multiclass classification.​\n",
        "\n",
        "**Strategies for Multiclass Classification**  \n",
        "**One-versus-the-rest (OvR)** or one-versus-all: Train 10 binary classifiers (one per digit 0-9), then select the class with the highest decision score.​\n",
        "\n",
        "**One-versus-one (OvO)**: Train a binary classifier for every pair of digits, requiring N × (N−1)/2 classifiers (45 for MNIST). When classifying, run the image through all classifiers and select the class winning the most duels. OvO's advantage is that each classifier trains only on the two classes it must distinguish.​\n",
        "\n",
        "For algorithms that scale poorly with training set size, OvO is preferred (faster to train many classifiers on small sets). For most binary classification algorithms, OvR is preferred.  \n",
        "\n",
        "**Training a Multiclass Classifier**  \n",
        "Scikit-Learn automatically runs OvR or OvO when using binary classifiers for multiclass tasks:"
      ],
      "metadata": {
        "id": "ARhoIlfyX6MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.svm import SVC\n",
        ">>> svm_clf = SVC()\n",
        ">>> svm_clf.fit(X_train, y_train) # y_train, not y_train_5\n",
        ">>> svm_clf.predict([some_digit])\n",
        "array([5], dtype=uint8)"
      ],
      "metadata": {
        "id": "pR_ixGhwYpyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SVC trains on original target classes 0-9, making a correct prediction. Scikit-Learn used OvO strategy, training 45 binary classifiers and selecting the class winning the most duels.​\n",
        "\n",
        "View decision scores:"
      ],
      "metadata": {
        "id": "i7ArrHiqYriz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> some_digit_scores = svm_clf.decision_function([some_digit])\n",
        ">>> some_digit_scores\n",
        "array([[ 2.92492871,  7.02307409,  3.93648529,  0.90117363,  5.96945908,\n",
        "         9.5       ,  1.90718593,  8.02755089, -0.13202708,  4.94216947]])\n",
        ">>> np.argmax(some_digit_scores)\n",
        "5\n",
        ">>> svm_clf.classes_\n",
        "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n",
        ">>> svm_clf.classes_[5]\n",
        "5"
      ],
      "metadata": {
        "id": "2BT7RuVTYtOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When trained, classifiers store target classes in the classes_ attribute, ordered by value.​\n",
        "\n",
        "Force OvR or OvO using OneVsRestClassifier or OneVsOneClassifier:"
      ],
      "metadata": {
        "id": "39gtbt3zYu0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.multiclass import OneVsRestClassifier\n",
        ">>> ovr_clf = OneVsRestClassifier(SVC())\n",
        ">>> ovr_clf.fit(X_train, y_train)\n",
        ">>> ovr_clf.predict([some_digit])\n",
        "array([5], dtype=uint8)\n",
        ">>> len(ovr_clf.estimators_)\n",
        "10"
      ],
      "metadata": {
        "id": "RoNXjIlZYwqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates 10 classifiers for OvR strategy.​\n",
        "\n",
        "Train SGDClassifier (natively multiclass):"
      ],
      "metadata": {
        "id": "lJQ73xmsYx-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> sgd_clf.fit(X_train, y_train)\n",
        ">>> sgd_clf.predict([some_digit])\n",
        "array([5], dtype=uint8)"
      ],
      "metadata": {
        "id": "PVD9aAnHYz4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn didn't need OvR or OvO because SGD classifiers directly classify into multiple classes:"
      ],
      "metadata": {
        "id": "zXKZxMuGY1TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> sgd_clf.decision_function([some_digit])\n",
        "array([[-15955.22628, -38080.96296, -13326.66695,   573.52692, -17680.68466,\n",
        "          2412.53175, -25526.86498, -12290.15705, -7946.05205, -10631.35889]])"
      ],
      "metadata": {
        "id": "9dWhYI1uY24r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier is confident (class 5 scores 2412.5) with slight doubt about class 3 (scores 573.5).​\n",
        "\n",
        "Evaluate using cross-validation:"
      ],
      "metadata": {
        "id": "3EE5C2fKY4Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
        "array([0.8489802 , 0.87129356, 0.86988048])"
      ],
      "metadata": {
        "id": "PYUrx0K5Y5-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over 84% accuracy beats the 10% random classifier baseline. Simply scaling inputs increases accuracy above 89%:"
      ],
      "metadata": {
        "id": "k05pyNtdY7Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.preprocessing import StandardScaler\n",
        ">>> scaler = StandardScaler()\n",
        ">>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
        ">>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n",
        "array([0.89707059, 0.8960948 , 0.90693604])"
      ],
      "metadata": {
        "id": "zYvNQ2vtY8j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---  \n",
        "\n",
        "**Error Analysis**  \n",
        "After finding a promising model, analyze the types of errors it makes to improve performance.​\n",
        "\n",
        "**Confusion Matrix Analysis**  \n",
        "Generate predictions and create confusion matrix:\n"
      ],
      "metadata": {
        "id": "iaezkgjHY--G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
        ">>> conf_mx = confusion_matrix(y_train, y_train_pred)\n",
        ">>> conf_mx\n",
        "array([[5578,    0,   22,    7,    8,   45,   35,    5,  222,    1],\n",
        "       [   0, 6410,   35,   26,    4,   44,    4,    8,  198,   13],\n",
        "       [  28,   27, 5232,  100,   74,   27,   68,   37,  354,   11],\n",
        "       [  23,   18,  115, 5254,    2,  209,   26,   38,  373,   73],\n",
        "       [  11,   14,   45,   12, 5219,   11,   33,   26,  299,  172],\n",
        "       [  26,   16,   31,  173,   54, 4484,   76,   14,  482,   65],\n",
        "       [  31,   17,   45,    2,   42,   98, 5556,    3,  123,    1],\n",
        "       [  20,   10,   53,   27,   50,   13,    3, 5696,  173,  220],\n",
        "       [  17,   64,   47,   91,    3,  125,   24,   11, 5421,   48],\n",
        "       [  24,   18,   29,   67,  116,   39,    1,  174,  329, 5152]])"
      ],
      "metadata": {
        "id": "konmOwkwZHcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize with Matplotlib:"
      ],
      "metadata": {
        "id": "jSvS3yACZJmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_PLvY8_tZLL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix looks good with most images on the main diagonal (correctly classified). The 5s appear slightly darker, potentially indicating fewer 5s in the dataset or worse classifier performance on 5s.​\n",
        "\n",
        "Focus on errors by normalizing the confusion matrix:"
      ],
      "metadata": {
        "id": "fcx-O4LMZMoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
        "norm_conf_mx = conf_mx / row_sums\n",
        "\n",
        "np.fill_diagonal(norm_conf_mx, 0)\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VlUGJMGGZOLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rows represent actual classes, columns represent predicted classes. The bright column for class 8 indicates many images get misclassified as 8s, but the row for class 8 isn't too bad (actual 8s are generally classified correctly). Classes 3 and 5 often get confused in both directions.​\n",
        "\n",
        "**Insights:** Focus efforts on reducing false 8s by gathering more training data for digits resembling 8s, engineering features (e.g., counting closed loops), or preprocessing images to make patterns stand out.  \n",
        "\n",
        "**Analyzing Individual Errors**\n",
        "Plot examples of 3s and 5s:"
      ],
      "metadata": {
        "id": "iijsHaVhZPZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl_a, cl_b = 3, 5\n",
        "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
        "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
        "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
        "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
        "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
        "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
        "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "quECM__lZXTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two 5×5 blocks on the left show digits classified as 3s; on the right, digits classified as 5s. Some misclassified digits are so badly written that even humans would struggle. However, most errors seem obvious to humans.​\n",
        "\n",
        "The SGDClassifier is a linear model assigning weights per class to each pixel, summing weighted pixel intensities to score each class. Since 3s and 5s differ by only a few pixels, the model easily confuses them. The main difference is the small line joining the top line to the bottom arc.​\n",
        "\n",
        "The classifier is quite sensitive to image shifting and rotation. Preprocessing images to ensure they're well-centered and not too rotated would reduce 3/5 confusion and other errors.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2G8hX3omZYtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multilabel Classification**\n",
        "\n",
        "**Multilabel classification** systems output multiple binary tags per instance. For example, a face-recognition classifier recognizing Alice, Bob, and Charlie should output for a picture containing Alice and Charlie.​\n",
        "\n",
        "Simple example:"
      ],
      "metadata": {
        "id": "8BqlDA0yZccR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "y_train_large = (y_train >= 7)\n",
        "y_train_odd = (y_train % 2 == 1)\n",
        "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
        "\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_train, y_multilabel)"
      ],
      "metadata": {
        "id": "KigzIWj0Zkof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This creates two target labels per digit: whether it's large (7, 8, or 9) and whether it's odd.​\n",
        "\n",
        "Make predictions:"
      ],
      "metadata": {
        "id": "yPXt9akWZnHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> knn_clf.predict([some_digit])\n",
        "array([[False,  True]])"
      ],
      "metadata": {
        "id": "FZpNoex3ZoWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct prediction: digit 5 is not large (False) and is odd (True).  \n",
        "\n",
        "**Evaluating Multilabel Classifiers**  \n",
        "One approach measures the F1 score for each individual label, then computes the average:"
      ],
      "metadata": {
        "id": "8vtgjtSCZpZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
        ">>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n",
        "0.976410265560605"
      ],
      "metadata": {
        "id": "CAWEq2EsZwjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assumes all labels are equally important. For weighted evaluation (e.g., more pictures of Alice than Bob), set average=\"weighted\" to give each label weight equal to its support.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sBtj84ZNZxxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multioutput Classification**  \n",
        "\n",
        "**Multioutput-multiclass **classification (multioutput classification) is a generalization of multilabel classification where each label can be multiclass (more than two possible values).​\n",
        "\n",
        "**Example: Noise Removal System**  \n",
        "Build a system removing noise from digit images. It takes noisy digit images as input and outputs clean images (arrays of pixel intensities). The output is multilabel (one label per pixel) where each label has multiple values (pixel intensity 0-255).​\n",
        "\n",
        "The line between classification and regression can be blurry—predicting pixel intensity is more akin to regression. Multioutput systems aren't limited to classification; they can output multiple labels per instance including both class labels and value labels.​\n",
        "\n",
        "Create noisy training and test sets:"
      ],
      "metadata": {
        "id": "JashF751Z0s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
        "X_train_mod = X_train + noise\n",
        "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
        "X_test_mod = X_test + noise\n",
        "y_train_mod = X_train\n",
        "y_test_mod = X_test"
      ],
      "metadata": {
        "id": "_NPQCRXWZ-Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target images are the original clean images.​\n",
        "\n",
        "Train and clean an image:"
      ],
      "metadata": {
        "id": "hETMYwlOZ_ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_clf.fit(X_train_mod, y_train_mod)\n",
        "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
        "plot_digit(clean_digit)"
      ],
      "metadata": {
        "id": "D_A_7G_taBhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cleaned output looks close to the target image."
      ],
      "metadata": {
        "id": "tjRZgfiPaCxd"
      }
    }
  ]
}