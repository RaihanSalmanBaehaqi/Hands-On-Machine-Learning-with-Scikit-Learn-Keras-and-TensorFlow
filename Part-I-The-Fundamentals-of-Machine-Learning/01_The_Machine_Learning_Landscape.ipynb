{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AUTHOR: RAIHAN SALMAN BAEHAQI (1103220180)**"
      ],
      "metadata": {
        "id": "j7CS6yHEhSd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART I**\n",
        "\n",
        "\n",
        "**The Fundamentals of Machine Learning**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wyvY9A2uZiDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**CHAPTER 1 - The Machine Learning Landscape**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5UaeARL2Y4UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is often associated with futuristic robots, but it's already an integral part of our lives. It has existed for decades in specialized fields like Optical Character Recognition (OCR) and became widely known in the 1990s through applications like spam filters, which have improved significantly over time. Today, ML powers various features such as recommendation systems and voice search.\n",
        "\n",
        "This chapter aims to clarify what ML is, exploring fundamental concepts such as the differences between supervised and unsupervised learning, online versus batch learning, and instance-based versus model-based learning. It also covers the typical workflow of an ML project, common challenges, and how to evaluate and optimize ML systems.\n",
        "\n",
        "The chapter provides a high-level introduction to the essential ML concepts and terminology that data scientists should master. While the content remains simple and without much code, it's crucial to understand these basics before diving deeper into the subject.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zei5-zg8ZIg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Is Machine Learning?**"
      ],
      "metadata": {
        "id": "b4cOjQh-ZPr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning is the science (and art) of programming computers so they can\n",
        "learn from data.\n",
        "\n",
        "Here is a slightly more general definition:\n",
        "\n",
        "\n",
        "> \"Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed\".\n",
        "\n",
        "Arthur Samuel, 1959\n",
        "\n",
        "And a more engineering-oriented one:\n",
        "> \"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E\".\n",
        "\n",
        "Tom Mitchell, 1997\n",
        "\n",
        "A spam filter is an example of Machine Learning, where the system learns to identify spam emails using a training set of labeled examples (spam and non-spam). The task is to flag spam, the experience is the training data, and performance is measured by accuracy, the ratio of correctly classified emails. Simply downloading a large dataset like Wikipedia does not qualify as Machine Learning because the system does not learn or improve at a specific task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XgY01vTkaRhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Use Machine Learning?**"
      ],
      "metadata": {
        "id": "v2m8vG8OaL6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To write a spam filter using traditional programming, you would:\n",
        "\n",
        "\n",
        "1.   Identify patterns in spam emails, like certain words or phrases (e.g., \"free,\" \"credit card\") in the subject line, sender‚Äôs name, and body.\n",
        "\n",
        "1.   Write algorithms to detect these patterns, flagging emails as spam when a certain number are found.\n",
        "\n",
        "1.   Test and refine the program, repeating steps 1 and 2 until it performs well enough to launch.\n",
        "\n",
        "This process relies on manually identifying and coding patterns, unlike Machine Learning, where the system learns patterns from data.\n",
        "\n",
        "![Figure1-1.jpg](./01.Chapter-01/Figure1-1.jpg)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YWjj01XqnZu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional spam filters rely on complex, manually written rules, making them hard to maintain and update. If spammers change their tactics, like replacing \"4U\" with \"For U,\" the filter would need constant updates to keep up.\n",
        "\n",
        "In contrast, a Machine Learning-based spam filter automatically learns which words and phrases predict spam by detecting patterns in data. It adapts to new patterns, such as \"For U,\" without needing manual updates, making it more efficient and accurate.\n",
        "\n",
        "![Figure1-2.jpg](./01.Chapter-01/Figure1-2.jpg)\n",
        "\n",
        "![Figure1-3.jpg](./01.Chapter-01/Figure1-3.jpg)"
      ],
      "metadata": {
        "id": "TmmLe7_8yZmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning is ideal for problems that are too complex for traditional methods or lack known algorithms, like speech recognition. While a simple approach might work for a few words, it doesn't scale for thousands of words or varied speakers in different environments. The solution is an algorithm that learns from examples.\n",
        "\n",
        "Additionally, Machine Learning can assist humans in learning by revealing patterns in data. For instance, a trained spam filter can show which words predict spam, uncovering hidden correlations. This process, called data mining, helps discover patterns that aren't immediately obvious.\n",
        "\n",
        "![Figure1-4.jpg](./01.Chapter-01/Figure1-4.jpg)"
      ],
      "metadata": {
        "id": "3Nj46QBo0utQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize, Machine Learning is great for:\n",
        "*   Problems for which existing solutions require a lot of fine-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform better than the traditional approach.\n",
        "*   Complex problems for which using a traditional approach yields no good solution: the best Machine Learning techniques can perhaps find a solution.\n",
        "*   Fluctuating environments: a Machine Learning system can adapt to new data.\n",
        "*   Getting insights about complex problems and large amounts of data.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CVqeghuL1al3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Examples of Applications**"
      ],
      "metadata": {
        "id": "j6xXKIo41-uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some examples of Machine Learning applications and the techniques used for them:\n",
        "* Image classification (e.g., classifying products on a production line): Convolutional Neural Networks (CNNs)\n",
        "* Tumor detection in brain scans: Semantic segmentation using CNNs\n",
        "* Classifying news articles: Natural Language Processing (NLP) with techniques like RNNs, CNNs, or Transformers\n",
        "* Flagging offensive comments: Text classification using NLP tools\n",
        "* Document summarization: Text summarization using NLP tools\n",
        "* Chatbots or personal assistants: NLP components like natural language understanding and question-answering modules\n",
        "* Revenue forecasting: Regression models, such as Linear Regression, Random Forest, or Neural Networks, often using RNNs or Transformers for past performance data\n",
        "* Voice command recognition: Speech recognition using RNNs, CNNs, or Transformers\n",
        "* Credit card fraud detection: Anomaly detection\n",
        "* Client segmentation for marketing: Clustering\n",
        "* Data visualization: Dimensionality reduction techniques\n",
        "* Product recommendation: Recommender systems using neural networks trained on past purchase data\n",
        "* Game bots: Reinforcement Learning (RL), like the AlphaGo program, to maximize rewards in a given environment\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ewbOXovB2-V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Machine Learning Systems**"
      ],
      "metadata": {
        "id": "r7-muPNq4DfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning systems can be classified into broad categories based on several criteria:\n",
        "\n",
        "1. Supervision:\n",
        "\n",
        "    * Supervised Learning: Trained with labeled data.\n",
        "\n",
        "    * Unsupervised Learning: Trained without labeled data.\n",
        "\n",
        "    * Semi-supervised Learning: A mix of both labeled and unlabeled data.\n",
        "\n",
        "    * Reinforcement Learning: Learns through trial and error to maximize rewards.\n",
        "\n",
        "2. Learning Type:\n",
        "\n",
        "    * Online Learning: Learns incrementally, updating as new data arrives.\n",
        "\n",
        "    * Batch Learning: Learns from a fixed dataset all at once.\n",
        "\n",
        "3. Learning Approach:\n",
        "\n",
        "    * Instance-based Learning: Compares new data points to known data points.\n",
        "\n",
        "    * Model-based Learning: Detects patterns in the data to build a predictive model."
      ],
      "metadata": {
        "id": "Jt0u6xOS4YlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised Learning**"
      ],
      "metadata": {
        "id": "jVwq3VcNDrhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In supervised learning, the training data includes both the input data and the corresponding labels (desired outputs).\n",
        "\n",
        "![Figure1-5.jpg](./01.Chapter-01/Figure1-5.jpg)\n",
        "\n",
        "* Classification: Tasks like spam detection, where emails are labeled as spam or ham.\n",
        "\n",
        "* Regression: Tasks like predicting the price of a car based on features like mileage and age.\n",
        "\n",
        "Key supervised learning algorithms:\n",
        "\n",
        "* k-Nearest Neighbors\n",
        "\n",
        "* Linear Regression\n",
        "\n",
        "* Logistic Regression\n",
        "\n",
        "* Support Vector Machines (SVMs)\n",
        "\n",
        "* Decision Trees and Random Forests\n",
        "\n",
        "* Neural Networks"
      ],
      "metadata": {
        "id": "-zQFQUMjF6Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised Learning**"
      ],
      "metadata": {
        "id": "Om6A1IKBGP8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In unsupervised learning, the training data is unlabeled, and the system tries to learn patterns without predefined labels.\n",
        "\n",
        "![Figure1-7.jpg](./01.Chapter-01/Figure1-7.jpg)\n",
        "\n",
        "* Clustering: Grouping similar data, e.g., clustering blog visitors into groups based on behavior (using algorithms like k-Means, DBSCAN, HCA).\n",
        "\n",
        "* Anomaly detection: Identifying rare events or outliers, such as fraudulent credit card transactions (using methods like One-class SVM, Isolation Forest).\n",
        "\n",
        "* Dimensionality reduction: Simplifying the data by combining correlated features, e.g., combining car mileage and age into one feature (using PCA, t-SNE).\n",
        "\n",
        "* Association rule learning: Discovering relationships between attributes, e.g., finding that people who buy barbecue sauce and chips often buy steak.\n",
        "\n",
        "In clustering, for example, the algorithm groups data based on similarities without being told which group each instance belongs to, making it useful for customer segmentation or pattern discovery.\n",
        "\n",
        "Key Techniques in Unsupervised Learning:\n",
        "\n",
        "* Clustering: k-Means, DBSCAN\n",
        "\n",
        "* Anomaly detection: One-class SVM, Isolation Forest\n",
        "\n",
        "* Dimensionality reduction: PCA, t-SNE\n",
        "\n",
        "* Association rule learning: Apriori, Eclat\n",
        "\n",
        "Unsupervised learning is useful for exploring data and finding patterns without requiring labels, making it suitable for tasks like grouping similar items, reducing data complexity, and detecting anomalies."
      ],
      "metadata": {
        "id": "OYBsCWI-GbUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semi-supervised learning**"
      ],
      "metadata": {
        "id": "h-7vswf_HzlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semi-supervised learning is a method used when there is a large amount of unlabeled data and only a small amount of labeled data. This approach combines both unsupervised and supervised learning techniques to make the most of the available data.\n",
        "\n",
        "![Figure1-11.jpg](./01.Chapter-01/Figure1-11.jpg)\n",
        "\n",
        "For example, in services like Google Photos, the system automatically clusters similar photos (unsupervised learning) to recognize faces. Once you label a few people, the system can then identify and label the same person across all photos, making it more efficient for tasks like photo searches.\n",
        "\n",
        "Most semi-supervised learning algorithms are hybrids of unsupervised methods (like clustering) and supervised methods (like classification). For instance, deep belief networks (DBNs) use unsupervised restricted Boltzmann machines (RBMs) followed by supervised fine-tuning, enabling them to work effectively with both labeled and unlabeled data."
      ],
      "metadata": {
        "id": "qa34QciFJELR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reinforcement Learning**"
      ],
      "metadata": {
        "id": "M5yQtthbJdaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent takes actions and receives rewards (or penalties) based on those actions. The goal is for the agent to learn the best strategy, called a policy, to maximize rewards over time.\n",
        "\n",
        "![Figure1-12.jpg](./01.Chapter-01/Figure1-12.jpg)\n",
        "\n",
        "For example, robots use RL to learn tasks like walking, and AlphaGo, developed by DeepMind, used RL to learn how to play the game of Go. AlphaGo analyzed millions of games and played against itself to refine its strategy. It applied the learned policy to defeat the world champion, with no further learning occurring during the championship games."
      ],
      "metadata": {
        "id": "wlpeAPHMJpsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch and Online Learning**"
      ],
      "metadata": {
        "id": "vLPffzl5Kv-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch learning**"
      ],
      "metadata": {
        "id": "JIsozsUbKfal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch learning is a machine learning approach where the system learns from the entire dataset at once. It is trained offline using all available data, and once trained, it is deployed into production without learning any further. If new data arises (e.g., a new type of spam), the system must be retrained from scratch with the updated dataset.\n",
        "\n",
        "While this method works well for static datasets, it can be time-consuming and resource-intensive, requiring significant computing power. Additionally, for systems needing to adapt quickly to new data (e.g., predicting stock prices), batch learning may not be ideal. It is typically used for less dynamic situations and when data is not too large. For more adaptive systems, incremental learning is a better solution."
      ],
      "metadata": {
        "id": "q3K6pjA3K-lD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Online learning**"
      ],
      "metadata": {
        "id": "jv8uSG2hKrz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Online learning trains a machine learning system incrementally by feeding it data instances one by one or in small batches, allowing the system to learn as new data arrives. This method is ideal for systems that need to adapt quickly to continuous data streams (e.g., stock prices) and those with limited computing resources, as it discards old data after learning from it.\n",
        "\n",
        "Online learning is also useful for large datasets that can't fit into a machine‚Äôs memory, allowing training in smaller chunks (called out-of-core learning). One key parameter is the learning rate, which controls how quickly the system adapts to new data. A high learning rate makes the system adapt rapidly but may forget older data, while a low learning rate slows learning but reduces sensitivity to noise.\n",
        "\n",
        "A challenge of online learning is that bad data can cause the system‚Äôs performance to decline over time. Monitoring the system and detecting abnormal data (using anomaly detection) can help mitigate this risk."
      ],
      "metadata": {
        "id": "ktjj8A68LOym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instance-Based Versus Model-Based Learning**"
      ],
      "metadata": {
        "id": "EkFME3CDMLrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instance-based learning**"
      ],
      "metadata": {
        "id": "8gC8AXrHPcnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instance-based learning is a simple approach where the system \"learns by heart\" from the examples it has been given. For instance, in a spam filter, it would flag emails that are identical to previously flagged ones. However, a more advanced version would flag emails that are similar to known spam emails, using a similarity measure (e.g., counting common words). This method generalizes new cases by comparing them to the most similar known examples. The system then classifies new instances based on these comparisons, like classifying a new instance as a \"triangle\" if most similar instances belong to that class.\n",
        "\n",
        "![Figure1-15.jpg](./01.Chapter-01/Figure1-15.jpg)"
      ],
      "metadata": {
        "id": "2xPsyQDlMyhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model-based learning**"
      ],
      "metadata": {
        "id": "JYW8culVPZWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model-based learning involves creating a model based on the training data and using it to make predictions.\n",
        "\n",
        "![Figure1-16.jpg](./01.Chapter-01/Figure1-16.jpg)\n",
        "\n",
        "Example: Predicting Life Satisfaction Based on GDP\n",
        "\n",
        "Let‚Äôs say we want to predict life satisfaction based on a country's GDP per capita. We use Linear Regression to model this relationship.\n",
        "\n",
        "Step 1: Collect and Organize Data\n",
        "\n",
        "We gather data from sources like the OECD for life satisfaction and the IMF for GDP per capita. An example dataset might look like this:\n",
        "\n",
        "![Table1-1.jpg](./01.Chapter-01/Table1-1.jpg)\n",
        "\n",
        "![Figure1-17.jpg](./01.Chapter-01/Figure1-17.jpg)\n",
        "\n",
        "Step 2: Select a Model (Linear Regression)\n",
        "\n",
        "We decide to use a linear model to predict life satisfaction based on GDP. The formula for a linear model is:\n",
        "\n",
        "![Eq1-1.jpg](./01.Chapter-01/Eq1-1.jpg)\n",
        "\n",
        "Where:\n",
        "\n",
        "* ùúÉ0 = is the intercept (constant value),\n",
        "\n",
        "* ùúÉ1 = is the slope (coefficient for GDP per capita).\n",
        "\n",
        "Step 3: Train the Model (Find ùúÉ0 and ùúÉ1)\n",
        "\n",
        "Using a training algorithm (like Linear Regression), we find the optimal values of ùúÉ0 and ùúÉ1 that best fit the data. In our case, after training the model, we get:\n",
        "\n",
        "![Eq1-2.jpg](./01.Chapter-01/Eq1-2.jpg)\n",
        "\n",
        "This means the model can predict life satisfaction by applying these values for the intercept and slope.\n",
        "\n",
        "Step 4: Make Predictions\n",
        "\n",
        "Now that we have the model, we can use it to predict life satisfaction for countries with known GDP per capita.\n",
        "\n",
        "For Cyprus, we look up the GDP per capita, which is $22,587. We then apply the model:\n",
        "\n",
        "![Eq1-3.jpg](./01.Chapter-01/Eq1-3.jpg)\n",
        "\n",
        "Calculating the result:\n",
        "\n",
        "![Eq1-4.jpg](./01.Chapter-01/Eq1-4.jpg)\n",
        "\n",
        "So, the predicted life satisfaction for Cyprus, based on its GDP per capita of $22,587, is 5.96.\n",
        "\n"
      ],
      "metadata": {
        "id": "JiyUrlTaPj5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python Code Example\n",
        "\n",
        "Here‚Äôs how you could implement this in Python using Scikit-Learn:"
      ],
      "metadata": {
        "id": "p5CBBNTxTote"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.linear_model\n",
        "\n",
        "# Load the data\n",
        "oecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\n",
        "gdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
        "                             encoding='latin1', na_values=\"n/a\")\n",
        "\n",
        "# Prepare the data\n",
        "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
        "X = np.c_[country_stats[\"GDP per capita\"]]\n",
        "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
        "\n",
        "# Visualize the data\n",
        "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
        "plt.show()\n",
        "\n",
        "# Select a linear model\n",
        "model = sklearn.linear_model.LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make a prediction for Cyprus\n",
        "X_new = [[22587]] # Cyprus's GDP per capita\n",
        "print(model.predict(X_new)) # outputs [[ 5.96242338]]"
      ],
      "metadata": {
        "id": "VeyCxGm4TqBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Main Challenges of Machine Learning**"
      ],
      "metadata": {
        "id": "0PxIva11UVB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insufficient Quantity of Training Data**"
      ],
      "metadata": {
        "id": "vp3rJxqFUWLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insufficient Quantity of Training Data is a challenge in Machine Learning. Unlike a toddler who can quickly learn to recognize an apple with a few examples, ML algorithms often require large amounts of data to learn effectively. For simple problems, thousands of examples are needed, and for complex tasks like image or speech recognition, millions of examples may be necessary. This can be mitigated if parts of an existing model are reused, but generally, more data leads to better performance."
      ],
      "metadata": {
        "id": "TA71SmCKY5Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nonrepresentative Training Data**"
      ],
      "metadata": {
        "id": "NTVGw9JeUZfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonrepresentative Training Data occurs when the data used to train a model does not accurately reflect the real-world scenarios the model will encounter. For example, if some countries are missing from the training data, the model's predictions can be skewed. Adding missing countries can significantly change the model, revealing that very rich countries are not necessarily happier than moderately rich ones, and some poor countries are happier than many rich countries.\n",
        "\n",
        "![Figure1-21.jpg](./01.Chapter-01/Figure1-21.jpg)\n",
        "\n",
        "Using nonrepresentative data leads to inaccurate predictions, especially for outliers (e.g., very poor or very rich countries). Achieving a representative training set is challenging, as even large datasets can have sampling bias if the sample method is flawed, which can lead to misleading results."
      ],
      "metadata": {
        "id": "GT0n0IExZXi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Poor-Quality Data**"
      ],
      "metadata": {
        "id": "7nZXPcZbUdn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poor-Quality Data can significantly hinder the performance of machine learning models. Errors, outliers, and noise in the data make it difficult for the system to detect meaningful patterns, leading to inaccurate predictions.\n",
        "\n",
        "Cleaning up the data is crucial, and data scientists often spend a lot of time on this task. For example:\n",
        "\n",
        "* Outliers: Discarding or correcting data points that are extreme or erroneous.\n",
        "\n",
        "* Missing features: Deciding how to handle missing data (e.g., ignoring, filling with the median, or creating models with and without the missing feature).\n",
        "\n",
        "Proper data cleaning ensures that the model learns from reliable and accurate data, improving its performance."
      ],
      "metadata": {
        "id": "6O6Ec7tVZ0Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Irrelevant Features**"
      ],
      "metadata": {
        "id": "1HMbJcxhUh0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Irrelevant Features in training data can negatively impact the performance of a machine learning model. If the data contains too many irrelevant features, the model may struggle to learn useful patterns.\n",
        "\n",
        "Feature engineering is the process of selecting and creating the most relevant features for training:\n",
        "\n",
        "* Feature selection: Choosing the most important features from existing data.\n",
        "\n",
        "* Feature extraction: Combining features to create more useful ones (e.g., using dimensionality reduction).\n",
        "\n",
        "* Creating new features: Gathering additional data to improve the model's predictive power.\n",
        "\n",
        "Carefully selecting and engineering features is essential for building an effective model."
      ],
      "metadata": {
        "id": "eCSQOnMkaNWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting the Training Data**"
      ],
      "metadata": {
        "id": "vhSe1GfyUld-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting the Training Data occurs when a model performs well on the training data but fails to generalize to new, unseen data. This happens when the model becomes too complex and fits the noise or random patterns in the training data, rather than the true underlying trends.\n",
        "\n",
        "For example, a complex model might learn irrelevant patterns, such as associating countries with names containing a \"w\" to higher life satisfaction, which is purely coincidental. This kind of model is overfitted and won't perform well on new data.\n",
        "\n",
        "To prevent overfitting, regularization is used to simplify the model. It constrains the model, reducing its complexity and helping it generalize better. A regularized model might not fit the training data perfectly, but it will perform better on unseen data.\n",
        "\n",
        "The degree of regularization is controlled by a hyperparameter, which must be set before training. A high regularization value leads to a simpler model with a lower chance of overfitting but may make it less accurate. Properly tuning hyperparameters is crucial for creating an effective machine learning model."
      ],
      "metadata": {
        "id": "VBA4dxsEbVb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Underfitting the Training Data**"
      ],
      "metadata": {
        "id": "Iya3t4ofUoOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underfitting the Training Data occurs when a model is too simple to capture the underlying patterns in the data. For example, using a linear model for a complex problem like predicting life satisfaction can lead to inaccurate predictions, even on the training data.\n",
        "\n",
        "To fix underfitting, you can:\n",
        "\n",
        "* Select a more powerful model with more parameters.\n",
        "\n",
        "* Improve the features (feature engineering) provided to the model.\n",
        "\n",
        "* Reduce constraints on the model, such as decreasing the regularization hyperparameter, to allow more flexibility.\n",
        "\n",
        "The goal is to create a model that is complex enough to learn the data's structure without overfitting."
      ],
      "metadata": {
        "id": "rKRZqFNtblGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stepping Back**\n"
      ],
      "metadata": {
        "id": "I72oITDRUssU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stepping Back provides a recap of the key points in Machine Learning:\n",
        "\n",
        "Machine Learning allows systems to improve at tasks by learning from data instead of relying on explicitly programmed rules.\n",
        "\n",
        "There are various types of ML systems, such as supervised vs unsupervised, batch vs online, and instance-based vs model-based.\n",
        "\n",
        "In an ML project, data is gathered in a training set, which is then used by a learning algorithm. A model-based algorithm adjusts parameters to fit the data, while an instance-based algorithm learns examples and generalizes based on similarity.\n",
        "\n",
        "For a system to perform well, the training data must be sufficient, representative, and clean. The model should be complex enough to capture patterns but simple enough to avoid overfitting.\n",
        "\n",
        "Finally, once the model is trained, it‚Äôs important to evaluate and fine-tune it to ensure it generalizes well to new, unseen data.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yodN3m1tb4C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing and Validating**"
      ],
      "metadata": {
        "id": "DtYdFd6xUxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing and Validating is essential to evaluate how well a model will generalize to new data. Instead of just using the model in production, a better method is to split the data into two sets: the training set (used to train the model) and the test set (used to evaluate the model's performance on new data).\n",
        "\n",
        "The generalization error (or out-of-sample error) is the error rate on the test set, which indicates how well the model will perform on unseen data. If the model has low training error but high generalization error, it suggests overfitting, meaning the model performs well on the training data but struggles with new data."
      ],
      "metadata": {
        "id": "iuQPXntKchAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning and Model Selection**"
      ],
      "metadata": {
        "id": "1a50S7rTVB0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning and Model Selection involves selecting the best model and setting its hyperparameters for optimal performance.\n",
        "\n",
        "To choose between models (e.g., linear vs. polynomial), you can train both and compare how well they generalize using a test set. For fine-tuning, you might train multiple models with different hyperparameter values and select the one with the lowest generalization error. However, adjusting hyperparameters based on test set performance can lead to overfitting on the test set, causing poor performance on new data.\n",
        "\n",
        "To avoid this, holdout validation is used: part of the training set is held out as a validation set to evaluate different models and hyperparameters. The best model is selected based on its performance on the validation set, then retrained on the full training set, and finally tested on the test set.\n",
        "\n",
        "Repeated cross-validation can further improve the process by using multiple small validation sets and averaging their performance, providing a more accurate estimate. However, this increases training time, as the model is evaluated multiple times."
      ],
      "metadata": {
        "id": "oGM6gHoSdDdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Mismatch**"
      ],
      "metadata": {
        "id": "ITaCc88-VDD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Mismatch occurs when the training data is not representative of the data that will be used in production. For example, if you're building a mobile app to identify flower species from pictures, you might have millions of web-sourced flower images for training, but they won't reflect the real pictures taken by users on the app.\n",
        "\n",
        "To address this, the validation and test sets must be as representative as possible of the actual data. One solution is to use a train-dev set to evaluate the model on data similar to what the app will encounter. If the model performs poorly on the validation set, it may be due to data mismatch. In this case, you can preprocess the web images to resemble those from the mobile app and retrain the model. If the model performs poorly on the train-dev set, it likely overfitted the training data, and adjustments like regularization or more diverse data may be needed."
      ],
      "metadata": {
        "id": "757nbXcEdToC"
      }
    }
  ]
}