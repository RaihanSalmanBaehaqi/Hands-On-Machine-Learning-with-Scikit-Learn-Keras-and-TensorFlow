{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AUTHOR: RAIHAN SALMAN BAEHAQI (1103220180)**"
      ],
      "metadata": {
        "id": "FDSvByVknq9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART I**  \n",
        "\n",
        "**The Fundamentals of Machine Learning**  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wMLM5sO7nwrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHAPTER 4 - Training Models**  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Lg6Y6v3XnzGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 4 delves into the inner workings of machine learning models, focusing on Linear Regression and its variants, Gradient Descent optimization, Polynomial Regression with regularization techniques, and classification models including Logistic and Softmax Regression.​  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jRUmF9T-n5o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression**  \n",
        "Linear Regression makes predictions by computing a weighted sum of input features plus a bias term (intercept).  \n",
        "\n",
        "Model Equations\n",
        "Equation 4-1. Linear Regression model prediction  \n",
        "![Eq4-1.jpg](./04.Chapter-04/Eq4-1.jpg)  \n",
        "\n",
        "Where:\n",
        "* $\\hat{y}$ is the predicted value\n",
        "* n is the number of features\n",
        "* x<sub>i</sub> is the ith feature value\n",
        "* θ<sub>j</sub> is the jth model parameter (including bias term θ<sub>0</sub> and feature weights θ<sub>1</sub>, θ<sub>2</sub>, ..., θ<sub>n</sub>)\n",
        "\n",
        "Equation 4-2. Linear Regression model prediction (vectorized form)  \n",
        "![Eq4-2.jpg](./04.Chapter-04/Eq4-2.jpg)  \n",
        "\n",
        "Where:\n",
        "* θ is the model's parameter vector containing bias and feature weights​\n",
        "* x is the instance's feature vector with x<sub>0</sub> always equal to 1​\n",
        "* θ⋅x is the dot product​\n",
        "* h<sub>θ</sub> is the hypothesis function using model parameters θ\n",
        "\n",
        "**Cost Function**  \n",
        "Training means minimizing the Mean Squared Error (MSE) rather than RMSE for mathematical convenience.​\n",
        "\n",
        "Equation 4-3. MSE cost function for a Linear Regression model  \n",
        "![Eq4-3.jpg](./04.Chapter-04/Eq4-3.jpg)\n",
        "\n"
      ],
      "metadata": {
        "id": "S8jQjD4XoqNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Normal Equation**  \n",
        "\n",
        "The closed-form solution directly computes optimal parameters.​\n",
        "\n",
        "Equation 4-4. Normal Equation  \n",
        "![Eq4-4.jpg](./04.Chapter-04/Eq4-4.jpg)  \n",
        "\n",
        "Where:\n",
        "* $\\hat{θ}$ is the value of θ that minimizes the cost function​\n",
        "* y is the vector of target values​\n",
        "\n",
        "Generate test data:"
      ],
      "metadata": {
        "id": "M1h6vkGNs2oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ],
      "metadata": {
        "id": "UfkxmrgYtZQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure4-1.jpg](./04.Chapter-04/Figure4-1.jpg)  \n",
        "\n",
        "Compute θ using the Normal Equation:"
      ],
      "metadata": {
        "id": "YqcdKoc8tbfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
        "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "\n",
        ">>> theta_best\n",
        "array([[4.21509616],\n",
        "       [2.77011339]])"
      ],
      "metadata": {
        "id": "StktpHUytjBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original function was y = 4 + 3x<sub>1</sub> + Gaussian noise and the algorithm found\n",
        "θ<sub>0</sub> = 4.215 θ<sub>1</sub> = 2.770  \n",
        "\n",
        "Make predictions:"
      ],
      "metadata": {
        "id": "KI7dSGDntlAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> X_new = np.array([[0], [2]])\n",
        ">>> X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
        ">>> y_predict = X_new_b.dot(theta_best)\n",
        ">>> y_predict\n",
        "array([[4.21509616],\n",
        "       [9.75532293]])"
      ],
      "metadata": {
        "id": "Y8_mXKc4uXE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot predictions:"
      ],
      "metadata": {
        "id": "O0WD5LXNuYSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X_new, y_predict, \"r-\")\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JIlBkrhKuajN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure4-2.jpg](./04.Chapter-04/Figure4-2.jpg)  \n",
        "\n",
        "**Using Scikit-Learn **\n",
        "Perform Linear Regression simply:"
      ],
      "metadata": {
        "id": "L1vdh8-oubwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.linear_model import LinearRegression\n",
        ">>> lin_reg = LinearRegression()\n",
        ">>> lin_reg.fit(X, y)\n",
        ">>> lin_reg.intercept_, lin_reg.coef_\n",
        "(array([4.21509616]), array([[2.77011339]]))\n",
        ">>> lin_reg.predict(X_new)\n",
        "array([[4.21509616],\n",
        "       [9.75532293]])"
      ],
      "metadata": {
        "id": "9JqFFPc6ul2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn separates the bias term (intercept_) from feature weights (coef_).  \n",
        "\n",
        "The LinearRegression class uses scipy.linalg.lstsq() (least squares):"
      ],
      "metadata": {
        "id": "C4QBvbI2unga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
        ">>> theta_best_svd\n",
        "array([[4.21509616],\n",
        "       [2.77011339]])"
      ],
      "metadata": {
        "id": "EQfvP6MLurmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or compute the pseudoinverse directly:"
      ],
      "metadata": {
        "id": "08W-xX8Yus5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> np.linalg.pinv(X_b).dot(y)\n",
        "array([[4.21509616],\n",
        "       [2.77011339]])"
      ],
      "metadata": {
        "id": "6FqmwtN1uuok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pseudoinverse is computed using Singular Value Decomposition (SVD) that decomposes X into UΣV<sup>T</sup>. The pseudoinverse is X<sup>+</sup> = V Σ<sup>+</sup> U<sup>T</sup>. This approach handles edge cases where X<sup>T</sup> X is not invertible (singular)."
      ],
      "metadata": {
        "id": "s1ZHh6xGuwFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computational Complexity**  \n",
        "The Normal Equation computes the inverse of an (n + 1) × (n+ 1) matrix with complexity O(n<sup>2.4</sup>) to O(n<sup>3</sup>). Doubling features multiplies computation time by roughly 5.3 to 8.​\n",
        "\n",
        "SVD approach is about O(n<sup>2</sup>). Both approaches get slow when features exceed 100,000 but are linear with regard to training instances O(m). Predictions are very fast: linear with both instances and features.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "L76q476hvssE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent**  \n",
        "Gradient Descent is a generic optimization algorithm that iteratively tweaks parameters to minimize a cost function.​\n",
        "\n",
        "The algorithm measures the local gradient of the error function with regard to parameter vector θ and moves in the direction of descending gradient. You start by filling θ with random values (random initialization), then improve it gradually until convergence.  \n",
        "\n",
        "![Figure4-3.jpg](./04.Chapter-04/Figure4-3.jpg)\n",
        "\n",
        "An important parameter is the learning rate hyperparameter that determines step size. If too small, the algorithm requires many iterations  \n",
        "\n",
        "![Figure4-4.jpg](./04.Chapter-04/Figure4-4.jpg)  \n",
        "\n",
        "If too high, you might jump across the valley and diverge  \n",
        "\n",
        "![Figure4-5.jpg](./04.Chapter-04/Figure4-5.jpg)  \n",
        "\n",
        "![Figure4-6.jpg](./04.Chapter-04/Figure4-6.jpg)  \n",
        "\n",
        "Fortunately, the MSE cost function for Linear Regression is convex (any line segment joining two points never crosses the curve). This means there are no local minima, just one global minimum, and it's a continuous function with slope that never changes abruptly. Gradient Descent is guaranteed to approach the global minimum arbitrarily close.  \n",
        "\n",
        "![Figure4-7.jpg](./04.Chapter-04/Figure4-7.jpg)  \n",
        "\n",
        "Important: When using Gradient Descent, ensure all features have similar scale (e.g., using StandardScaler), or convergence will take much longer.\n"
      ],
      "metadata": {
        "id": "yULQ2OaVwcSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Gradient Descent**  \n",
        "Compute partial derivatives of the cost function with regard to each parameter θ<sub>j</sub>.\n",
        "\n",
        "Equation 4-5. Partial derivatives of the cost function  \n",
        "![Eq4-5.jpg](./04.Chapter-04/Eq4-5.jpg)  \n",
        "\n",
        "Equation 4-6. Gradient vector of the cost function  \n",
        "![Eq4-6.jpg](./04.Chapter-04/Eq4-6.jpg)  \n",
        "\n",
        "This formula involves calculations over the full training set X at each step, which is why it's called Batch Gradient Descent. It's terribly slow on very large training sets but scales well with number of features.​  \n",
        "\n",
        "Equation 4-7. Gradient Descent step  \n",
        "![Eq4-7.jpg](./04.Chapter-04/Eq4-7.jpg)  \n",
        "\n",
        "Where η (eta) is the learning rate.​\n",
        "\n",
        "Quick implementation:"
      ],
      "metadata": {
        "id": "2mi_qg2GxLTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        ">>> theta\n",
        "array([[4.21509616],\n",
        "       [2.77011339]])"
      ],
      "metadata": {
        "id": "EQrv4Y3zxyxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result matches the Normal Equation exactly.  \n",
        "\n",
        "![Figure4-8.jpg](./04.Chapter-04/Figure4-8.jpg)  \n",
        "\n",
        "To set the number of iterations, use a very large number but interrupt when the gradient vector becomes tiny (norm smaller than tolerance ϵ).  \n",
        "\n",
        "**Convergence Rate**  \n",
        "With a fixed learning rate, Batch Gradient Descent will eventually converge to the optimal solution, but it can take O(1/ϵ) iterations. If you divide tolerance by 10 for more precision, the algorithm may run about 10 times longer."
      ],
      "metadata": {
        "id": "CDoDVerYx1YV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic Gradient Descent**  \n",
        "The main problem with Batch GD is using the whole training set at every step, making it very slow when the training set is large. Stochastic Gradient Descent picks a random instance at every step and computes gradients based only on that single instance.​\n",
        "\n",
        "Working on a single instance makes the algorithm much faster and enables training on huge datasets as only one instance needs to be in memory. It can be implemented as an out-of-core algorithm.​\n",
        "\n",
        "Due to its random nature, the cost function bounces up and down, decreasing only on average. It will end up very close to the minimum but continue bouncing around, never settling down  \n",
        "![Figure4-9.jpg](./04.Chapter-04/Figure4-9.jpg)  \n",
        "\n",
        "This randomness helps the algorithm jump out of local minima, giving it a better chance of finding the global minimum than Batch GD. One solution is to gradually reduce the learning rate using a learning schedule. Steps start out large (quick progress, escape local minima), then get smaller, allowing the algorithm to settle at the global minimum.​\n",
        "\n",
        "Implementation:"
      ],
      "metadata": {
        "id": "352mtNutyNkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "\n",
        ">>> theta\n",
        "array([[4.21076011],\n",
        "       [2.74856079]])"
      ],
      "metadata": {
        "id": "XAwWNlISyl_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By convention, we iterate by rounds of m iterations; each round is called an epoch.  \n",
        "\n",
        "![Figure4-10.jpg](./04.Chapter-04/Figure4-10.jpg)  \n",
        "\n",
        "**Important**: Training instances must be independent and identically distributed (IID). Shuffle instances during training to ensure parameters get pulled toward the global optimum on average. If not shuffled (e.g., sorted by label), SGD will optimize for one label at a time and not settle close to the global minimum.​\n",
        "\n",
        "Using Scikit-Learn's SGDRegressor:"
      ],
      "metadata": {
        "id": "iXTlOgLTynnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "\n",
        ">>> sgd_reg.intercept_, sgd_reg.coef_\n",
        "(array([4.24365286]), array([2.8250878]))"
      ],
      "metadata": {
        "id": "4VcS2AYFy5Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mini-batch Gradient Descent**  \n",
        "**Mini-batch Gradient Descent** computes gradients on small random sets of instances called mini-batches. The main advantage over Stochastic GD is performance boost from hardware optimization of matrix operations, especially with GPUs.​\n",
        "\n",
        "The algorithm's progress is less erratic than Stochastic GD and walks closer to the minimum, but it may be harder to escape local minima.  \n",
        "\n",
        "![Figure4-11.jpg](./04.Chapter-04/Figure4-11.jpg)  \n",
        "\n",
        "Algorithm Comparison  \n",
        "\n",
        "Table 4-1. Comparison of algorithms for Linear Regression  \n",
        "![Table4-1.jpg](./04.Chapter-04/Table4-1.jpg)  \n",
        "\n",
        "Where m is number of training instances and n is number of features. After training, all algorithms end up with very similar models and make predictions identically.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7DRxgEGby7Ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Polynomial Regression**  \n",
        "\n",
        "For data more complex than a straight line, add powers of each feature as new features, then train a linear model on this extended set.​\n",
        "\n",
        "Generate nonlinear data based on a quadratic equation:"
      ],
      "metadata": {
        "id": "rtMdLkdVziov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
      ],
      "metadata": {
        "id": "0VIRZfp6zsyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure4-12.jpg](./04.Chapter-04/Figure4-12.jpg)  \n",
        "\n",
        "Transform training data:"
      ],
      "metadata": {
        "id": "Gt1nDktvzufz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.preprocessing import PolynomialFeatures\n",
        ">>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        ">>> X_poly = poly_features.fit_transform(X)\n",
        ">>> X[0]\n",
        "array([-0.75275929])\n",
        ">>> X_poly[0]\n",
        "array([-0.75275929, 0.56664654])"
      ],
      "metadata": {
        "id": "sduBo_Qjzynw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_poly now contains the original feature plus its square.​\n",
        "\n",
        "Fit a LinearRegression model:"
      ],
      "metadata": {
        "id": "rAmULYIoz0R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> lin_reg = LinearRegression()\n",
        ">>> lin_reg.fit(X_poly, y)\n",
        ">>> lin_reg.intercept_, lin_reg.coef_\n",
        "(array([1.78134581]), array([[0.93366893, 0.56456263]]))"
      ],
      "metadata": {
        "id": "pP52PczTz3R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure4-13.jpg](./04.Chapter-04/Figure4-13.jpg)  \n",
        "\n",
        "With multiple features, PolynomialFeatures adds all combinations up to the given degree. For two features a and b, degree=3 would add a<sup>2</sup>, a<sup>3</sup>, b<sup>2</sup>, b<sup>3</sup>, ab, a<sup>2</sup>b, ab<sup>2</sup>.  \n",
        "\n",
        "Warning: PolynomialFeatures(degree=d) transforms an array containing n features into (n+d)!/d!n! features. Beware of combinatorial explosion.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4rjHx6cDz4ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Curves**  \n",
        "High-degree Polynomial Regression fits training data much better than plain Linear Regression.  \n",
        "\n",
        "![Figure4-14.jpg](./04.Chapter-04/Figure4-14.jpg)  \n",
        "\n",
        "Learning curves are plots of model performance on training and validation sets as a function of training set size.​\n",
        "\n",
        "Function to plot learning curves:"
      ],
      "metadata": {
        "id": "jOSL8hVu1Kzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")"
      ],
      "metadata": {
        "id": "DrUPjcOo1UOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot for Linear Regression:"
      ],
      "metadata": {
        "id": "_z4H4X951Wa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X, y)"
      ],
      "metadata": {
        "id": "lAiTJnyb1YEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure4-15.jpg](./04.Chapter-04/Figure4-15.jpg)  \n",
        "\n",
        "**Key insight**: If your model is underfitting, adding more training examples will not help. You need a more complex model or better features.  \n",
        "\n",
        "10th-degree polynomial model:"
      ],
      "metadata": {
        "id": "il5jDbP71aKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "plot_learning_curves(polynomial_regression, X, y)"
      ],
      "metadata": {
        "id": "usnesEb51jpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure4-16.jpg](./04.Chapter-04/Figure4-16.jpg)  \n",
        "\n",
        "**Key insight**: One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error.  \n",
        "\n",
        "**The Bias/Variance Trade-off**  \n",
        "A model's generalization error can be expressed as the sum of three errors:​\n",
        "\n",
        "**Bias**: Due to wrong assumptions (e.g., assuming data is linear when it's quadratic). High-bias models likely underfit training data.​\n",
        "\n",
        "**Variance**: Due to excessive sensitivity to small variations in training data. Models with many degrees of freedom (high-degree polynomials) have high variance and overfit.​\n",
        "\n",
        "**Irreducible error**: Due to data noisiness. Reduce this by cleaning data (fix sensors, remove outliers).​\n",
        "\n",
        "Increasing model complexity typically increases variance and reduces bias. Conversely, reducing complexity increases bias and reduces variance.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6LlyPvA61l_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularized Linear Models**  \n",
        "A good way to reduce overfitting is to regularize the model (constrain it). Fewer degrees of freedom make it harder to overfit. For polynomial models, reduce polynomial degrees. For linear models, constrain the weights."
      ],
      "metadata": {
        "id": "bnZtWozZ19aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression**  \n",
        "**Ridge Regression** (Tikhonov regularization) adds a regularization term $\n",
        "\\alpha \\sum_{i=1}^{n} \\theta_i^2\n",
        "$ to the cost function. This forces the learning algorithm to fit the data and keep model weights small.​\n",
        "\n",
        "**Important**: The regularization term should only be added during training. Use the unregularized performance measure for testing.​\n",
        "\n",
        "The hyperparameter α controls regularization strength. If α = 0, Ridge Regression is just Linear Regression. If α is very large, all weights end up near zero (flat line through data's mean).  \n",
        "\n",
        "Equation 4-8. Ridge Regression cost function  \n",
        "![Eq4-8.jpg](./04.Chapter-04/Eq4-8.jpg)  \n",
        "\n",
        "The bias term θ<sub>0</sub> is not regularized. The regularization term equalsals 1/2 (∥w∥<sub>2</sub>)<sup>2</sup>, where ∥w∥<sub>2</sub> is the ℓ2 norm of the weight vector  \n",
        "\n",
        "Important: Scale the data (e.g., using StandardScaler) before performing Ridge Regression, as it's sensitive to input feature scale. This is true for most regularized models.  \n",
        "\n",
        "![Figure4-17.jpg](./04.Chapter-04/Figure4-17.jpg)  \n",
        "\n",
        "Equation 4-9. Ridge Regression closed-form solution  \n",
        "![Eq4-9.jpg](./04.Chapter-04/Eq4-9.jpg)  \n",
        "\n",
        "Where A is the (n+1)×(n+1) identity matrix with a 0 in the top-left cell (corresponding to bias term).​\n",
        "\n",
        "Using Scikit-Learn with closed-form solution:"
      ],
      "metadata": {
        "id": "_b85KKgj2DLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.linear_model import Ridge\n",
        ">>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
        ">>> ridge_reg.fit(X, y)\n",
        ">>> ridge_reg.predict([[1.5]])\n",
        "array([[1.55071465]])"
      ],
      "metadata": {
        "id": "_yHjC4tB4y9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Stochastic Gradient Descent:"
      ],
      "metadata": {
        "id": "BG-01AjX41hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> sgd_reg = SGDRegressor(penalty=\"l2\")\n",
        ">>> sgd_reg.fit(X, y.ravel())\n",
        ">>> sgd_reg.predict([[1.5]])\n",
        "array([1.47012588])"
      ],
      "metadata": {
        "id": "aewM1mCN43k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The penalty hyperparameter sets the regularization term type. Specifying \"l2\" adds half the square of the ℓ2 norm of the weight vector (Ridge Regression)."
      ],
      "metadata": {
        "id": "a9_QsaiH4551"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Regression**  \n",
        "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) adds a regularization term using the ℓ1 norm of the weight vector.  \n",
        "\n",
        "Equation 4-10. Lasso Regression cost function  \n",
        "![Eq4-10.jpg](./04.Chapter-04/Eq4-10.jpg)  \n",
        "\n",
        "![Figure4-18.jpg](./04.Chapter-04/Figure4-18.jpg)  \n",
        "\n",
        "An important characteristic: Lasso tends to eliminate weights of least important features (set them to zero). For example, the dashed line with α=10<sup>−7</sup> looks quadratic, almost linear—all high-degree polynomial feature weights equal zero. Lasso automatically performs feature selection and outputs a sparse model.  \n",
        "\n",
        "![Figure4-19.jpg](./04.Chapter-04/Figure4-19.jpg)  \n",
        "\n",
        "Important: To avoid bouncing around the optimum with Lasso, gradually reduce the learning rate during training.​\n",
        "\n",
        "The Lasso cost function is not differentiable at θ<sub>i</sub> = 0, but Gradient Descent works fine using a subgradient vector.  \n",
        "\n",
        "Equation 4-11. Lasso Regression subgradient vector  \n",
        "![Eq4-11.jpg](./04.Chapter-04/Eq4-11.jpg)  \n",
        "\n",
        "Using Scikit-Learn:"
      ],
      "metadata": {
        "id": "m-39PQJ_49Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.linear_model import Lasso\n",
        ">>> lasso_reg = Lasso(alpha=0.1)\n",
        ">>> lasso_reg.fit(X, y)\n",
        ">>> lasso_reg.predict([[1.5]])\n",
        "array([1.53788174])"
      ],
      "metadata": {
        "id": "1i3h7b4u5zps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elastic Net**  \n",
        "**Elastic Net** is a middle ground between Ridge and Lasso. The regularization term is a mix of both, controlled by mix ratio r. When r=0, Elastic Net is equivalent to Ridge; when r=1, it's equivalent to Lasso.  \n",
        "\n",
        "Equation 4-12. Elastic Net cost function  \n",
        "![Eq4-12.jpg](./04.Chapter-04/Eq4-12.jpg)  \n",
        "\n",
        "When to use which:\n",
        "* It's almost always preferable to have at least a little regularization, so avoid plain Linear Regression​\n",
        "* Ridge is a good default​\n",
        "* Lasso or Elastic Net if you suspect only a few features are useful (they reduce useless feature weights to zero)​\n",
        "* Elastic Net is preferred over Lasso because Lasso may behave erratically when features outnumber training instances or when several features are strongly correlated​\n",
        "\n",
        "Using Scikit-Learn:"
      ],
      "metadata": {
        "id": "Bpyxfh8453ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn.linear_model import ElasticNet\n",
        ">>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        ">>> elastic_net.fit(X, y)\n",
        ">>> elastic_net.predict([[1.5]])\n",
        "array([1.54333232])"
      ],
      "metadata": {
        "id": "hlykRdmc6PyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early Stopping**  \n",
        "A different way to regularize iterative learning algorithms: stop training as soon as validation error reaches a minimum.  \n",
        "\n",
        "![Figure4-20.jpg](./04.Chapter-04/Figure4-20.jpg)  \n",
        "\n",
        "**Important**: With Stochastic and Mini-batch GD, curves aren't smooth. Stop only after validation error has been above the minimum for some time, then roll back parameters to where validation error was minimum.​\n",
        "\n",
        "Basic implementation:"
      ],
      "metadata": {
        "id": "fymT1Ft36R0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone\n",
        "\n",
        "# prepare the data\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler())\n",
        "    ])\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error:\n",
        "        minimum_val_error = val_error\n",
        "        best_epoch = epoch\n",
        "        best_model = clone(sgd_reg)"
      ],
      "metadata": {
        "id": "q-bxodnY6eAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With warm_start=True, fit() continues training where it left off instead of restarting.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pIvpxyPv6hlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**  \n",
        "Logistic Regression (Logit Regression) is commonly used to estimate the probability that an instance belongs to a particular class. If estimated probability > 50%, the model predicts the instance belongs to that class (positive class, labeled \"1\"); otherwise it predicts it doesn't (negative class, labeled \"0\"). This makes it a binary classifier.​"
      ],
      "metadata": {
        "id": "VlSMTwDi6k9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimating Probabilities**  \n",
        "Just like Linear Regression, Logistic Regression computes a weighted sum of input features (plus bias), but instead of outputting the result directly, it outputs the logistic of this result.  \n",
        "\n",
        "Equation 4-13. Logistic Regression model estimated probability (vectorized form)  \n",
        "![Eq4-13.jpg](./04.Chapter-04/Eq4-13.jpg)  \n",
        "\n",
        "The logistic—noted σ(⋅)—is a sigmoid function (S-shaped) that outputs a number between 0 and 1.  \n",
        "\n",
        "Equation 4-14. Logistic function  \n",
        "![Eq4-14.jpg](./04.Chapter-04/Eq4-14.jpg)  \n",
        "\n",
        "![Figure4-21.jpg](./04.Chapter-04/Figure4-21.jpg)  \n",
        "\n",
        "Once the model has estimated probability $\\hat{p}$ = h<sub>θ</sub>(x), it can make predictions easily.  \n",
        "\n",
        "Equation 4-15. Logistic Regression model prediction  \n",
        "![Eq4-15.jpg](./04.Chapter-04/Eq4-15.jpg)  \n",
        "\n",
        "Notice that σ(t)<0.5 when t<0, and σ(t)≥0.5 when t≥0, so Logistic Regression predicts 1 if x<sup>T</sup>θ is positive and 0 if negative.​\n",
        "\n",
        "The score t is often called the logit. The name comes from the logit function logit (p)= log(p/(1−p)), which is the inverse of the logistic function. The logit is also called the log-odds (log of the ratio between estimated probability for positive class and estimated probability for negative class).\n"
      ],
      "metadata": {
        "id": "DuiNzVyO6vy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Cost Function**  \n",
        "\n",
        "The training objective is to set parameter vector θ so the model estimates high probabilities for positive instances (y=1) and low probabilities for negative instances (y=0).  \n",
        "\n",
        "Equation 4-16. Cost function of a single training instance  \n",
        "![Eq4-16.jpg](./04.Chapter-04/Eq4-16.jpg)  \n",
        "\n",
        "This makes sense because −log(t) grows very large when t approaches 0. The cost will be large if the model estimates a probability close to 0 for a positive instance or close to 1 for a negative instance.  \n",
        "\n",
        "Equation 4-17. Logistic Regression cost function (log loss)  \n",
        "![Eq4-17.jpg](./04.Chapter-04/Eq4-17.jpg)  \n",
        "\n",
        "There is no closed-form equation to compute θ that minimizes this cost function. However, the cost function is convex, so Gradient Descent is guaranteed to find the global minimum.  \n",
        "\n",
        "Equation 4-18. Logistic cost function partial derivatives  \n",
        "![Eq4-18.jpg](./04.Chapter-04/Eq4-18.jpg)  \n",
        "\n",
        "This equation looks very much like the Linear Regression gradient: for each instance it computes the prediction error and multiplies it by the jth feature value, then computes the average over all training instances."
      ],
      "metadata": {
        "id": "vcQXk9KA8xgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Boundaries**  \n",
        "Use the iris dataset to illustrate Logistic Regression. The dataset contains sepal and petal length and width of 150 iris flowers of three different species: Iris setosa, Iris versicolor, and Iris virginica.  \n",
        "\n",
        "![Figure4-22.jpg](./04.Chapter-04/Figure4-22.jpg)  \n",
        "\n",
        "Build a classifier to detect Iris virginica based only on petal width:"
      ],
      "metadata": {
        "id": "Bu2gzdeD9h3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from sklearn import datasets\n",
        ">>> iris = datasets.load_iris()\n",
        ">>> list(iris.keys())\n",
        "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']\n",
        ">>> X = iris[\"data\"][:, 3:]  # petal width\n",
        ">>> y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
      ],
      "metadata": {
        "id": "AJR8M5sX9q2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a Logistic Regression model:"
      ],
      "metadata": {
        "id": "iPT3Umhe9tA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X, y)"
      ],
      "metadata": {
        "id": "ITpKCldv9vGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at estimated probabilities for petal widths from 0 cm to 3 cm:"
      ],
      "metadata": {
        "id": "pAa2-n1R9xCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris virginica\")"
      ],
      "metadata": {
        "id": "uOYyzu1m9zGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Figure4-23.jpg](./04.Chapter-04/Figure4-23.jpg)  \n",
        "\n",
        "Make predictions:"
      ],
      "metadata": {
        "id": "Mhy8-I7390gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> log_reg.predict([[1.7], [1.5]])\n",
        "array([1, 0])"
      ],
      "metadata": {
        "id": "6UNVAA6-97NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display two features: petal width and length.  \n",
        "\n",
        "![Figure4-24.jpg](./04.Chapter-04/Figure4-24.jpg)  \n",
        "\n",
        "Logistic Regression models can be regularized using ℓ1 or ℓ2 penalties. Scikit-Learn adds an ℓ2 penalty by default.​\n",
        "\n",
        "**Important**: The hyperparameter controlling regularization strength is not alpha but its inverse: C. The higher the value of C, the less the model is regularized."
      ],
      "metadata": {
        "id": "fss0cke198zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Regression\n",
        "Logistic Regression can be generalized to support multiple classes directly, without training and combining multiple binary classifiers. This is called Softmax Regression or Multinomial Logistic Regression.​\n",
        "\n",
        "When given an instance x, the model first computes a score s<sub>k</sub>(x) for each class k, then estimates the probability of each class by applying the softmax function to the scores.  \n",
        "\n",
        "Equation 4-19. Softmax score for class k  \n",
        "![Eq4-19.jpg](./04.Chapter-04/Eq4-19.jpg)  \n",
        "\n",
        "Each class has its own dedicated parameter vector θ<sup>(k)</sup>. All vectors are typically stored as rows in a parameter matrix Θ.  \n",
        "\n",
        "Equation 4-20. Softmax function  \n",
        "![Eq4-20.jpg](./04.Chapter-04/Eq4-20.jpg)  \n",
        "\n",
        "Where:\n",
        "* K is the number of classes​\n",
        "* s(x) is a vector containing scores of each class for instance x\n",
        "* σ(s(x))<sub>k</sub> is the estimated probability that instance x belongs to class k​\n",
        "\n",
        "Scores are generally called logits or log-odds (unnormalized).  \n",
        "\n",
        "![Eq4-21.jpg](./04.Chapter-04/Eq4-21.jpg)  \n",
        "\n",
        "The argmax operator returns the value of a variable that maximizes a function. The classifier predicts only one class at a time (multiclass, not multioutput).​\n",
        "\n",
        "**Important**: Use only with mutually exclusive classes (different types of plants). Cannot use to recognize multiple people in one picture.​\n",
        "\n",
        "The training objective is to have a model that estimates a high probability for the target class and low probability for other classes.  \n",
        "\n",
        "Equation 4-22. Cross entropy cost function  \n",
        "![Eq4-22.jpg](./04.Chapter-04/Eq4-22.jpg)  \n",
        "\n",
        "Where:\n",
        "* y<sub>k</sub><sup>(i)</sup> is the target probability that the ith instance belongs to class k. Generally equal to 1 or 0 depending on whether the instance belongs to the class​\n",
        "\n",
        "When there are just two classes (K=2), this is equivalent to Logistic Regression's log loss.  \n",
        "\n",
        "**Cross Entropy**  \n",
        "Cross entropy originated from information theory. If you want to efficiently transmit weather information with eight options, you could encode each using three bits (2<sup>3</sup> = 8). However, if \"sunny\" is most common, it's more efficient to code \"sunny\" with one bit (0) and other seven options with four bits (starting with 1). Cross entropy measures the average number of bits you actually send per option.​\n",
        "\n",
        "If your assumption is perfect, cross entropy equals the entropy of the weather itself (intrinsic unpredictability). If assumptions are wrong, cross entropy will be greater by an amount called the Kullback-Leibler (KL) divergence.​\n",
        "\n",
        "Cross entropy between two probability distributions p and q is defined as H(p,q)=−∑<sub>x</sub> p(x) log q(x).  \n",
        "\n",
        "Equation 4-23. Cross entropy gradient vector for class k  \n",
        "![Eq4-23jpg](./04.Chapter-04/Eq4-23.jpg)  \n",
        "\n",
        "Compute the gradient vector for every class, then use Gradient Descent to find parameter matrix Θ that minimizes the cost function.​\n",
        "\n",
        "**Using Scikit-Learn**  \n",
        "Classify iris flowers into all three classes. LogisticRegression uses one-versus-the-rest by default for more than two classes, but you can set multi_class to \"multinomial\" to switch to Softmax Regression. Must specify a solver supporting Softmax Regression like \"lbfgs\". It applies ℓ2 regularization by default, controlled by hyperparameter C:"
      ],
      "metadata": {
        "id": "z8xJQYj1-HbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
        "softmax_reg.fit(X, y)"
      ],
      "metadata": {
        "id": "d1Fuz5oCAxjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make predictions:"
      ],
      "metadata": {
        "id": "Co9bvZm3Az79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> softmax_reg.predict([[5, 2]])\n",
        "array([2])\n",
        ">>> softmax_reg.predict_proba([[5, 2]])\n",
        "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])"
      ],
      "metadata": {
        "id": "q6TdEyyIA2lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For an iris with 5 cm long and 2 cm wide petals, the model predicts Iris virginica (class 2) with 94.2% probability (or Iris versicolor with 5.8% probability).  \n",
        "\n",
        "![Figure4-25.jpg](./04.Chapter-04/Figure4-25.jpg)\n"
      ],
      "metadata": {
        "id": "clomXuLrA5oI"
      }
    }
  ]
}